sample <- sample.int(n = nrow(ccancer_clean), size = floor(.75*nrow(ccancer_clean)), replace = F)
train <- ccancer_clean[sample, ]
test  <- ccancer_clean[-sample, ]
summarytools::dfSummary(na.omit(train), graph.col = F, valid.col = F)
Oil.and.Gas.1932.2014 <- read.csv("~/OneDriveUni/adv_mic_mac/macro/macro_project/Oil and Gas 1932-2014.csv")
View(Oil.and.Gas.1932.2014)
oil_gas <- read.csv("~/OneDriveUni/adv_mic_mac/macro/macro_project/Oil and Gas 1932-2014.csv")
table(oil_gas$eiacty)
View(oil_gas)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(ggplot2)
library(dplyr)
library(reshape2)
library(mlr3learners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
library(broom)
library(ltm)
library(MASS)
wwines <- read.csv("winequality-white.csv", sep=";")
print("White wines")
str(wwines)
wwines$quality <- as.factor(case_when(wwines$quality <= 5 ~ "low",
wwines$quality > 5 ~ "high"))
table(wwines$quality)
residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
density.plot <- ggplot(wwines, aes(density, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)
#residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
#  geom_freqpoly(binwidth = 3, size = 1)
residual.sugar.plot <- ggplot(wwines, aes(x=residual.sugar, fill=quality)) +
geom_density(alpha=0.4)
alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
density.plot <- ggplot(wwines, aes(density, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)
#residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
#  geom_freqpoly(binwidth = 3, size = 1)
residual.sugar.plot <- ggplot(wwines, aes(x=residual.sugar, fill=quality)) +
geom_density(alpha=0.4)
alcohol.plot <- ggplot(wwines, aes(x=alcohol, fill=quality)) +
geom_density(alpha=0.4)
density.plot <- ggplot(wwines, aes(x=density, fill=quality)) +
geom_density(alpha=0.4)
#alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
#  geom_freqpoly(binwidth = 3, size = 1)
#
#density.plot <- ggplot(wwines, aes(density, color = quality))+
#  geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)
residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
density.plot <- ggplot(wwines, aes(density, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)
#residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
#  geom_freqpoly(binwidth = 3, size = 1)
residual.sugar.plot <- ggplot(wwines, aes(x=residual.sugar, fill=quality)) +
geom_density(alpha=0.4)
alcohol.plot <- ggplot(wwines, aes(x=alcohol, fill=quality)) +
geom_density(alpha=0.4)
density.plot <- ggplot(wwines, aes(x=density, fill=quality)) +
geom_density(alpha=0.4)
#alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
#  geom_freqpoly(binwidth = 3, size = 1)
#
#density.plot <- ggplot(wwines, aes(density, color = quality))+
#  geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)
residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
density.plot <- ggplot(wwines, aes(density, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)
wwines$density <- NULL
# melt the dataset
wwines.m <- melt(wwines[,-12], id.var = "quality")
# wrapped boxplots
ggplot(data = wwines.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=quality)) +
facet_wrap( ~ variable, scales="free")
set.seed(42)
train_indices<- createDataPartition(wwines$quality,p=0.7,list=FALSE)
dtrain <- wwines[train_indices,]
dtest <- wwines[-train_indices,]
getwd()
library(leaps)
regfit.fwd=regsubsets(quality~.,data=wwines[,-12],method="forward", nvmax=13)
summary.fwd <- summary(regfit.fwd)
plot(summary.fwd$bic,xlab="Number of Variables",ylab="bic")
points(which.min(summary.fwd$bic),summary.fwd$bic[which.min(summary.fwd$bic)],pch=20,col="red")
plot(regfit.fwd,scale="bic")
grid(nx=11,ny=10,col="red",lty = "solid")
abline(h=10, col="yellow", lwd = 10)
text(10.5,9, "Optimal model", col="yellow",cex = 0.8)
# relevel
wwines$quality <- factor(wwines$quality, levels = c("low","high"))
# remove unused variables
dtrain.res <- dtrain[,c(1,2,4,6,9,10,11,12)]
# relevel
wwines$quality <- factor(wwines$quality, levels = c("low","high"))
# remove unused variables
dtrain.res <- dtrain[,c(1,2,4,6,9,10,11)]
dtest.res <- dtest[,c(1,2,4,6,9,10,11)]
# train the model
lr_fit_res <- glm(quality ~., data =  dtrain.res[,-8],
family=binomial(link='logit'))
# coefficients
summary(lr_fit_res)
# odds ratios
exp(cbind(OR = coef(lr_fit_res), confint(lr_fit_res)))
# task: classification of training data
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
# type of learner: logistic regression
learner_train = lrn("classif.log_reg", predict_type = "prob")
# resampling method: cross validation
cv5 = rsmp("cv", folds = 5)
# defing the search space
thresholds <- seq(0.5,0.8,0.01)
#collecting cross-validated performances for different thresholds
measures_list <- rep(list(list()), 3)
for (thresh in thresholds) {
res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
#combined prediction of all individual resampling iterations
prediction <- res_cv$prediction()
prediction$set_threshold(thresh)
#computing the scores
scores <- prediction$score(measures = c(msr("classif.acc"),msr("classif.sensitivity"),msr("classif.specificity")))
coefficients <- (unname(scores))
measures_list <- mapply(append, measures_list, coefficients, SIMPLIFY = FALSE)
}
measures <- data.frame(thresholds,
"accuracy" = unlist(measures_list[[1]]),
"sensitivity" = unlist(measures_list[[2]]),
"specificity" = unlist(measures_list[[3]]))
#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
th <- mean(thresholds[intersection_indices])
melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) +
geom_line() +
geom_vline(xintercept = th,linetype = "dotted") +
geom_label(aes(x = 0.67, y = 0.5, label = as.character(mean(thresholds[intersection_indices]))))
# task: classification of training data
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
# type of learner: logistic regression
learner_train = lrn("classif.log_reg", predict_type = "prob")
# resampling method: cross validation
cv5 = rsmp("cv", folds = 5)
# defing the search space
thresholds <- seq(0.5,0.8,0.01)
#collecting cross-validated performances for different thresholds
measures_list <- rep(list(list()), 3)
for (thresh in thresholds) {
res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
#combined prediction of all individual resampling iterations
prediction <- res_cv$prediction()
prediction$set_threshold(thresh)
#computing the scores
scores <- prediction$score(measures = c(msr("classif.acc"),msr("classif.sensitivity"),msr("classif.specificity")))
coefficients <- (unname(scores))
measures_list <- mapply(append, measures_list, coefficients, SIMPLIFY = FALSE)
}
# task: classification of training data
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
# type of learner: logistic regression
learner_train = lrn("classif.log_reg", predict_type = "prob")
# resampling method: cross validation
cv5 = rsmp("cv", folds = 5)
# defing the search space
thresholds <- seq(0.5,0.8,0.01)
#collecting cross-validated performances for different thresholds
measures_list <- rep(list(list()), 3)
for (thresh in thresholds) {
res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
#combined prediction of all individual resampling iterations
prediction <- res_cv$prediction()
prediction$set_threshold(thresh)
#computing the scores
scores <- prediction$score(measures = c(msr("classif.acc"),msr("classif.sensitivity"),msr("classif.specificity")))
coefficients <- (unname(scores))
measures_list <- mapply(append, measures_list, coefficients, SIMPLIFY = FALSE)
}
measures <- data.frame(thresholds,
"accuracy" = unlist(measures_list[[1]]),
"sensitivity" = unlist(measures_list[[2]]),
"specificity" = unlist(measures_list[[3]]))
#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
th <- mean(thresholds[intersection_indices])
melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) +
geom_line() +
geom_vline(xintercept = th,linetype = "dotted") +
geom_label(aes(x = 0.67, y = 0.5, label = as.character(mean(thresholds[intersection_indices]))))
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
task_test = TaskClassif$new(id = "wines_test",  dtest.res[,-8], target = "quality", positive = "high")
learner = lrn("classif.log_reg", predict_type = "prob")
cv5 = rsmp("cv", folds = 5)
learner$train(task_train) # training on train set
base_pred <- learner$predict(task_test) # predicting on test set
base_pred$set_threshold(th)
# performance
cm_base <- list("confusion" = base_pred$confusion,
"accuracy" = base_pred$score(measures = msr("classif.acc")),
"sensitivity"=base_pred$score(measures = msr("classif.sensitivity")),
"specificity"=base_pred$score(measures = msr("classif.specificity")))
cm_base
#running the full model
lr_fit <- glm(quality ~., data =  dtrain[,-12],
family=binomial(link='logit'))
#full model performance
unrestr_probs <- predict(lr_fit,  dtest[,-12], type="response")
unrestr_preds <- ifelse(unrestr_probs > th,"high","low")
unrestr_cm <- confusionMatrix(factor(unrestr_preds),
dtest$quality,
positive = "high"
)
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
#restricted model performance
restr_probs <- predict(lr_fit_res,  dtest.res[,-8], type="response")
restr_preds <- ifelse(restr_probs > th,"high","low")
restr_cm <- confusionMatrix(factor(restr_preds),                         dtest$quality,
positive = "high"
)
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])
data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance),
"difference" = unlist(restr_performance)-unlist(unrestr_performance))
a <- vif(lr_fit_res)
sqrt(vif(lr_fit_res))>2
#see Cook's distance
plot(lr_fit_res, which = 4, id.n = 1)
dtrain_of <- dtrain.res %>% filter(id != "4746")
dtrain_of <- dtrain.res %>% dplyr::filter(id != "4746")
dtrain_of <- dtrain.res %>% filter(id = "4746")
dtrain_of <- dtrain.res %>% filter("id" != "4746")
dtrain_of <- dtrain.res %>% filter("id" != "4746")
lr_fit_of_1 <- glm(quality ~., data =  dtrain_of[,-8],
family=binomial(link='logit'))
plot(lr_fit_of_1, which = 4, id.n = 0)
dtrain_of <- dtrain.res %>% filter(id != "4746")
wwines["id"] <- rownames(wwines)
cors <- cor(wwines[-c(12,13)])
wwines$quality <- as.factor(case_when(wwines$quality <= 5 ~ "low",
wwines$quality > 5 ~ "high"))
table(wwines$quality)
wwines <- read.csv("winequality-white.csv", sep=";")
print("White wines")
str(wwines)
wwines$quality <- as.factor(case_when(wwines$quality <= 5 ~ "low",
wwines$quality > 5 ~ "high"))
table(wwines$quality)
wwines["id"] <- rownames(wwines)
cors <- cor(wwines[-c(12,13)])
corrplot(cors, type = "upper",tl.col = "black", tl.srt = 45)
residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
density.plot <- ggplot(wwines, aes(density, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)
wwines$density <- NULL
wwines$quality <- factor(wwines$quality, levels = c("high","low"))
bicorrs <- sapply(wwines[,-c(11,12)], function(x) round(biserial.cor(x,wwines$quality),2))
data.frame(bicorrs) %>% arrange(abs(bicorrs))
# melt the dataset
wwines.m <- melt(wwines[,-12], id.var = "quality")
# wrapped boxplots
ggplot(data = wwines.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=quality)) +
facet_wrap( ~ variable, scales="free")
set.seed(42)
train_indices<- createDataPartition(wwines$quality,p=0.7,list=FALSE)
dtrain <- wwines[train_indices,]
dtest <- wwines[-train_indices,]
regfit.fwd=regsubsets(quality~.,data=wwines[,-12],method="forward", nvmax=13)
summary.fwd <- summary(regfit.fwd)
plot(summary.fwd$bic,xlab="Number of Variables",ylab="bic")
points(which.min(summary.fwd$bic),summary.fwd$bic[which.min(summary.fwd$bic)],pch=20,col="red")
plot(regfit.fwd,scale="bic")
grid(nx=11,ny=10,col="red",lty = "solid")
abline(h=10, col="yellow", lwd = 10)
text(10.5,9, "Optimal model", col="yellow",cex = 0.8)
# relevel
wwines$quality <- factor(wwines$quality, levels = c("low","high"))
# remove unused variables
dtrain.res <- dtrain[,c(1,2,4,6,9,10,11,12)]
dtest.res <- dtest[,c(1,2,4,6,9,10,11,12)]
# train the model
lr_fit_res <- glm(quality ~., data =  dtrain.res[,-8],
family=binomial(link='logit'))
# coefficients
summary(lr_fit_res)
# odds ratios
exp(cbind(OR = coef(lr_fit_res), confint(lr_fit_res)))
# task: classification of training data
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
# type of learner: logistic regression
learner_train = lrn("classif.log_reg", predict_type = "prob")
# resampling method: cross validation
cv5 = rsmp("cv", folds = 5)
# defing the search space
thresholds <- seq(0.5,0.8,0.01)
#collecting cross-validated performances for different thresholds
measures_list <- rep(list(list()), 3)
for (thresh in thresholds) {
res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
#combined prediction of all individual resampling iterations
prediction <- res_cv$prediction()
prediction$set_threshold(thresh)
#computing the scores
scores <- prediction$score(measures = c(msr("classif.acc"),msr("classif.sensitivity"),msr("classif.specificity")))
coefficients <- (unname(scores))
measures_list <- mapply(append, measures_list, coefficients, SIMPLIFY = FALSE)
}
measures <- data.frame(thresholds,
"accuracy" = unlist(measures_list[[1]]),
"sensitivity" = unlist(measures_list[[2]]),
"specificity" = unlist(measures_list[[3]]))
#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
th <- mean(thresholds[intersection_indices])
melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) +
geom_line() +
geom_vline(xintercept = th,linetype = "dotted") +
geom_label(aes(x = 0.67, y = 0.5, label = as.character(mean(thresholds[intersection_indices]))))
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
task_test = TaskClassif$new(id = "wines_test",  dtest.res[,-8], target = "quality", positive = "high")
learner = lrn("classif.log_reg", predict_type = "prob")
cv5 = rsmp("cv", folds = 5)
learner$train(task_train) # training on train set
base_pred <- learner$predict(task_test) # predicting on test set
base_pred$set_threshold(th)
# performance
cm_base <- list("confusion" = base_pred$confusion,
"accuracy" = base_pred$score(measures = msr("classif.acc")),
"sensitivity"=base_pred$score(measures = msr("classif.sensitivity")),
"specificity"=base_pred$score(measures = msr("classif.specificity")))
cm_base
#running the full model
lr_fit <- glm(quality ~., data =  dtrain[,-12],
family=binomial(link='logit'))
#full model performance
unrestr_probs <- predict(lr_fit,  dtest[,-12], type="response")
unrestr_preds <- ifelse(unrestr_probs > th,"high","low")
unrestr_cm <- confusionMatrix(factor(unrestr_preds),
dtest$quality,
positive = "high"
)
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
#restricted model performance
restr_probs <- predict(lr_fit_res,  dtest.res[,-8], type="response")
restr_preds <- ifelse(restr_probs > th,"high","low")
restr_cm <- confusionMatrix(factor(restr_preds),                         dtest$quality,
positive = "high"
)
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])
data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance),
"difference" = unlist(restr_performance)-unlist(unrestr_performance))
a <- vif(lr_fit_res)
sqrt(vif(lr_fit_res))>2
# plot Cook's distance
plot(lr_fit_res, which = 4, id.n = 1)
dtrain_of <- dtrain.res %>% filter(id != "4746")
lr_fit_of_1 <- glm(quality ~., data =  dtrain_of[,-8],
family=binomial(link='logit'))
plot(lr_fit_of_1, which = 4, id.n = 0)
#fitting on outlier free data
lr_fit_outlierfree <- glm(quality ~., data =  dtrain_of[,-8],family=binomial(link='logit'))
# coefficients
summary(lr_fit_outlierfree)
# odds ratios
exp(cbind(OR = coef(lr_fit_outlierfree), confint(lr_fit_outlierfree)))
task_train = TaskClassif$new(id = "wines_outlierfree",  dtrain_of[,-8], target = "quality", positive = "high")
learner = lrn("classif.log_reg", predict_type = "prob")
cv5 = rsmp("cv", folds = 5)
task_test = TaskClassif$new(id = "wines_outlierfree_test",  dtest.res[,-8], target = "quality", positive = "high")
learner$train(task_train)
outlierfree_pred <- learner$predict(task_test)
# set optimal threshold
outlierfree_pred$set_threshold(th)
cm_outlierfree <- list("confusion" = outlierfree_pred$confusion,
"accuracy" = outlierfree_pred$score(measures = msr("classif.acc")),
"sensitivity"=outlierfree_pred$score(measures = msr("classif.sensitivity")),
"specificity"=outlierfree_pred$score(measures = msr("classif.specificity")))
cm_outlierfree
data.frame("outlierfree_performance" = unlist(cm_outlierfree[-1]),"base_performance" = unlist(cm_base[-1]),
"improvement" = unlist(cm_outlierfree[-1])-unlist(cm_base[-1]))
base <- data.frame(summary(lr_fit_res)$coefficient)
no_outliers<- data.frame(summary(lr_fit_outlierfree)$coefficient)
variable<-c("(constant)",colnames(dtrain.res)[-c(7,8)])
data.frame(variable,base$Estimate,no_outliers$Estimate, "difference"=base$Estimate-no_outliers$Estimate)
n <- seq(nrow(dtrain_of))
set.seed(1)
lr_b <- function(i){
set.seed(i)
s = sample(n, nrow(dtrain_of), replace = TRUE)
lr_f <- glm(quality ~ fixed.acidity+volatile.acidity+residual.sugar+free.sulfur.dioxide+sulphates+alcohol,
data =  dtrain_of[s, ],family=binomial(link='logit'))
m = as.data.frame(lr_f$coefficients)
return(m)
}
co <- data.frame("sub1" = lr_b(1), "sub2" = lr_b(2), "sub3" = lr_b(3), "sub4" = lr_b(4), "sub5" = lr_b(5), "sub6" = lr_b(6), "sub7" = lr_b(7),"sub8" = lr_b(8),"sub9" = lr_b(9),"sub10" = lr_b(10))
co$mean <- rowMeans(co)
pro <- function(i){ # predict the probability
l_p = co$mean[1]+co$mean[2]*dtest.res[i,1]+co$mean[3]*dtest.res[i,2]+co$mean[4]*dtest.res[i,3]+co$mean[5]*dtest.res[i,4]+co$mean[6]*dtest.res[i,5]+ co$mean[7]*dtest.res[i,6] # linear predictor
lr_b_prob = exp(l_p)/(1+exp(l_p))
lr_b_pred <- ifelse(lr_b_prob < th,"high","low")
return(lr_b_prob)
}
pre <- function(i){
lr_b_pred <- ifelse(pro(i) < th,"high","low")
return(lr_b_pred)
}
pred <- numeric()
prob <- numeric()
for (i in rownames(dtest.res)){
p = pro(i)
prob[i] <- p
q = pre(i)
pred[i] <- q
}
cm_b<-confusionMatrix(as.factor(pred),dtest.res$quality, positive = "high")
cm_b
dtest.res$prediction <- pred
dtest.res$probability <- prob
dtest.res$label <- as.numeric(case_when(dtest.res$quality == "low" ~ 1, dtest.res$quality == "high"  ~ 0))
# ROC
library(precrec)
precrec_obj <- evalmod(scores = dtest.res$probability, labels = dtest.res$label)
autoplot(precrec_obj)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(reshape2)
library(mlr3learners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
library(broom)
library(ltm)
library(MASS)
library(leaps)
library(ROCit)
wwines <- read.csv("winequality-white.csv", sep=";")
str(wwines)
wwines$quality <- factor(case_when(wwines$quality <= 5 ~ "low",
wwines$quality > 5 ~ "high"), levels = c("low", "high")) # relevel
table(wwines$quality)
wwines$quality <- factor(case_when(wwines$quality <= 5 ~ "low",
wwines$quality > 5 ~ "high"), levels = c("low", "high")) # relevel
table(wwines$quality)
wwines <- read.csv("winequality-white.csv", sep=";")
str(wwines)
wwines$quality <- factor(case_when(wwines$quality <= 5 ~ "low",
wwines$quality > 5 ~ "high"), levels = c("low", "high")) # relevel
table(wwines$quality)
wwines["id"] <- rownames(wwines)
cors <- cor(wwines[-c(12,13)])
corrplot(cors, type = "upper",tl.col = "black", tl.srt = 45)
residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
density.plot <- ggplot(wwines, aes(density, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)
bicorrs <- sapply(wwines[,-c(11,12)], function(x) round(biserial.cor(x,wwines$quality),2))
wwines$density <- NULL
bicorrs <- sapply(wwines[,-c(11,12)], function(x) round(biserial.cor(x,wwines$quality),2))
data.frame(bicorrs) %>% arrange(abs(bicorrs))
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(reshape2)
library(mlr3learners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
library(broom)
library(ltm)
library(MASS)
library(leaps)
library(ROCit)
wwines <- read.csv("winequality-white.csv", sep=";")
str(wwines)
