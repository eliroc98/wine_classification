knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(ggplot2)
library(dplyr)
library(reshape2)
library(mlr3learners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
wwines <- read.csv("winequality-white.csv", sep=";")
print("White wines")
str(wwines)
wwines$quality <- as.factor(case_when(wwines$quality <= 5 ~ "low",
wwines$quality > 5 ~ "high"))
wwines$quality <- factor(wwines$quality, levels = c("low","high"))
table(wwines$quality)
#compute on all features (without output variable which is at index 12)
cors <- cor(wwines[-12])
corrplot(cors, type = "upper",
tl.col = "black", tl.srt = 45)
h1 <- ggplot(wwines, aes(residual.sugar, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
h2 <- ggplot(wwines, aes(alcohol, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
h3 <- ggplot(wwines, aes(density, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(h1,h2,h3, ncol=1)
wwines$density <- NULL
h1 <- ggplot(wwines, aes(residual.sugar, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
h2 <- ggplot(wwines, aes(alcohol, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
h3 <- ggplot(wwines, aes(density, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(h1,h2,h3, ncol=1)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(ggplot2)
library(dplyr)
library(reshape2)
library(mlr3learners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
wwines <- read.csv("winequality-white.csv", sep=";")
print("White wines")
str(wwines)
wwines$quality <- as.factor(case_when(wwines$quality <= 5 ~ "low",
wwines$quality > 5 ~ "high"))
wwines$quality <- factor(wwines$quality, levels = c("low","high"))
table(wwines$quality)
#compute on all features (without output variable which is at index 12)
cors <- cor(wwines[-12])
corrplot(cors, type = "upper",
tl.col = "black", tl.srt = 45)
h1 <- ggplot(wwines, aes(residual.sugar, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
h2 <- ggplot(wwines, aes(alcohol, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
h3 <- ggplot(wwines, aes(density, color = quality))+
geom_freqpoly(binwidth = 3, size = 1)
grid.arrange(h1,h2,h3, ncol=1)
wwines$density <- NULL
library(ltm)
install.packages("ltm")
library(ltm)
for (i in 1:length(wwines)) {
print(paste(colnames(wwines)[i],biserial.cor(wwines[,i],wwines$quality),sep = " "))
}
library(ltm)
for (i in 1:length(wwines)) {
print(paste(colnames(wwines)[i],biserial.cor(wwines[,i],wwines$quality),sep = " "))
}
library(ltm)
for (i in 1:length(wwines[-12])) {
print(paste(colnames(wwines)[i],biserial.cor(wwines[,i],wwines$quality),sep = " "))
}
library(ltm)
for (i in 1:length(wwines[,-12])) {
print(paste(colnames(wwines)[i],biserial.cor(wwines[,i],wwines$quality),sep = " "))
}
library(ltm)
for (i in 1:length(wwines[,-11])) {
print(paste(colnames(wwines)[i],biserial.cor(wwines[,i],wwines$quality),sep = " "))
}
summary(wwines)
boxplot(wwines$fixed.acidity ~ wwines$quality)
boxplot(wwines$volatile.acidity ~ wwines$quality)
boxplot(wwines$citric.acid ~ wwines$quality)
boxplot(wwines$residual.sugar ~ wwines$quality)
boxplot(wwines$chlorides ~ wwines$quality)
boxplot(wwines$free.sulfur.dioxide ~ wwines$quality)
boxplot(wwines$total.sulfur.dioxide ~ wwines$quality)
boxplot(wwines$density ~ wwines$quality)
summary(wwines)
boxplot(wwines$fixed.acidity ~ wwines$quality)
boxplot(wwines$volatile.acidity ~ wwines$quality)
boxplot(wwines$citric.acid ~ wwines$quality)
boxplot(wwines$residual.sugar ~ wwines$quality)
boxplot(wwines$chlorides ~ wwines$quality)
boxplot(wwines$free.sulfur.dioxide ~ wwines$quality)
boxplot(wwines$total.sulfur.dioxide ~ wwines$quality)
boxplot(wwines$pH ~ wwines$quality)
boxplot(wwines$sulphates ~ wwines$quality)
boxplot(wwines$alcohol ~ wwines$quality)
set.seed(42)
train_indices<- createDataPartition(wwines$quality,p=0.7,list=FALSE)
lr_fit <- glm(quality ~., data =  wwines[train_indices,],
family=binomial(link='logit'))
# coefficients
summary(lr_fit)
# odds ratios
exp(cbind(OR = coef(lr_fit), confint(lr_fit)))
#here we use mlr3 package
#install_learners('classif.log_reg')
#mlr_learners$get("classif.log_reg")
#defining our task: classification on wines with quality as response variable and low quality as baseline
task_train = TaskClassif$new(id = "wines",  wwines[train_indices,], target = "quality", positive = "low")
#defining the learner which will perform the training procedure on our model
learner_train = lrn("classif.log_reg", predict_type = "prob")
#defining how we want to perform cross validation (setting number of folds to 5)
cv5 = rsmp("cv", folds = 5)
#we want to tweak the threshold of our classification: we analyze values from 0.3 to 0.6 with step 0.01
thresholds <- seq(0.3,0.6,0.01)
#collecting performances with different thresholds
accuracy <- list()
sensitivity <- list()
specificity <- list()
auroc <- list()
i <- 1
for (thresh in thresholds) {
res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
#combined prediction of all individual resampling iterations
prediction <- res_cv$prediction()
prediction$set_threshold(thresh)
#scores are combined as well from all individual resampling iterations
accuracy[i] <- prediction$score(measures = msr("classif.acc"), task = task_train)
sensitivity[i] <- prediction$score(measures = msr("classif.sensitivity"), task = task_train)
specificity[i] <- prediction$score(measures = msr("classif.specificity"), task = task_train)
auroc[i] <- prediction$score(measures = msr("classif.auc"), task = task_train)
i<-i+1
}
#collecting scores
measures <- data.frame(thresholds,
"accuracy" =unlist(accuracy),
"sensitivity" =unlist(sensitivity),
"specificity" = unlist(specificity),
"auroc" = unlist(auroc))
#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) +
geom_line() +
geom_vline(xintercept = mean(thresholds[intersection_indices]),linetype = "dotted") +
geom_label(aes(x = 0.37, y = 0.5, label = "0.355"))
# define new task for  wwines[-train_indices,]
task_test = TaskClassif$new(id = "wines_test",  wwines[-train_indices,], target = "quality", positive = "low")
# train on train task
learner_train$train(task_train)
# predict on test task
pred <- learner_train$predict(task_test)
# set optimal threshold
pred$set_threshold(mean(thresholds[intersection_indices]))
# performance
cm <- list("confusion" = pred$confusion,
"accuracy" = pred$score(measures = msr("classif.acc")),
"sensitivity"=pred$score(measures = msr("classif.sensitivity")),
"specificity"=pred$score(measures = msr("classif.specificity")))
cm
a <- vif(lr_fit)
#b <- data.frame(a,colnames(wwines)[-12])  %>% mutate(sqrt = sqrt(a)) %>% filter(sqrt > 2)
sqrt(vif(lr_fit))>2 # residual.sugar, density, alcohol
regfit.fwd=regsubsets(quality~.,data=wwines,method="forward", nvmax=13)
?regsubsets
??regsubsets
library(car)
regfit.fwd=regsubsets(quality~.,data=wwines,method="forward", nvmax=13)
library(broom)
regfit.fwd=regsubsets(quality~.,data=wwines,method="forward", nvmax=13)
library(leaps)
regfit.fwd=regsubsets(quality~.,data=wwines,method="forward", nvmax=13)
summary.fwd <- summary(regfit.fwd)
plot(summary.fwd$bic,xlab="Number of Variables",ylab="bic")
points(which.min(summary.fwd$bic),summary.fwd$bic[which.min(summary.fwd$bic)],pch=20,col="red")
plot(regfit.fwd,scale="bic")
grid(nx=11,ny=10,col="red",lty = "solid")
abline(h=10, col="yellow", lwd = 10)
text(10.5,9, "Optimal model", col="yellow",cex = 0.8)
lr_fit_res <- glm(quality ~ fixed.acidity+volatile.acidity+residual.sugar+free.sulfur.dioxide+sulphates+alcohol, data =  wwines[train_indices,],
family=binomial(link='logit'))
# coefficients
summary(lr_fit_res)
# odds ratios
exp(cbind(OR = coef(lr_fit_res), confint(lr_fit_res)))
#unrestricted model performance
unrestr_probs <- predict(lr_fit,  wwines[-train_indices,], type="response")
unrestr_preds <- ifelse(unrestr_probs > 0.355,"low","high")
unrestr_cm <- confusionMatrix(as.factor(unrestr_preds),
wwines[-train_indices,]$quality,
positive = "low"
)
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
#restricted model performance
restr_probs <- predict(lr_fit_res,  wwines[-train_indices,], type="response")
restr_preds <- ifelse(restr_probs > 0.355,"low","high")
restr_cm <- confusionMatrix(as.factor(restr_preds),
wwines[-train_indices,]$quality,
positive = "low"
)
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])
data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance),
"improvement" = unlist(restr_performance)-unlist(unrestr_performance))
as.factor(restr_preds)
wwines[-train_indices,]$quality
#unrestricted model performance
unrestr_probs <- predict(lr_fit,  wwines[-train_indices,], type="response")
unrestr_preds <- ifelse(unrestr_probs > 0.355,"low","high")
unrestr_cm <- confusionMatrix(as.factor(unrestr_preds),
wwines[-train_indices,]$quality,
positive = "low"
)
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
#restricted model performance
restr_probs <- predict(lr_fit_res,  wwines[-train_indices,], type="response")
restr_preds <- ifelse(restr_probs > 0.355,"low","high")
restr_cm <- confusionMatrix(as.factor(restr_preds,level=c("low","high")),
wwines[-train_indices,]$quality,
positive = "low"
)
#unrestricted model performance
unrestr_probs <- predict(lr_fit,  wwines[-train_indices,], type="response")
unrestr_preds <- ifelse(unrestr_probs > 0.355,"low","high")
unrestr_cm <- confusionMatrix(as.factor(unrestr_preds),
wwines[-train_indices,]$quality,
positive = "low"
)
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
#restricted model performance
restr_probs <- predict(lr_fit_res,  wwines[-train_indices,], type="response")
restr_preds <- ifelse(restr_probs > 0.355,"low","high")
restr_cm <- confusionMatrix(as.factor(restr_preds,levels(c("low","high"))),
wwines[-train_indices,]$quality,
positive = "low"
)
#unrestricted model performance
unrestr_probs <- predict(lr_fit,  wwines[-train_indices,], type="response")
unrestr_preds <- ifelse(unrestr_probs > 0.355,"low","high")
unrestr_cm <- confusionMatrix(as.factor(unrestr_preds),
wwines[-train_indices,]$quality,
positive = "low"
)
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
#restricted model performance
restr_probs <- predict(lr_fit_res,  wwines[-train_indices,], type="response")
restr_preds <- ifelse(restr_probs > 0.355,"low","high")
restr_cm <- confusionMatrix(as.factor(restr_preds,levels("low","high")),
wwines[-train_indices,]$quality,
positive = "low"
)
#unrestricted model performance
unrestr_probs <- predict(lr_fit,  wwines[-train_indices,], type="response")
unrestr_preds <- ifelse(unrestr_probs > 0.355,"low","high")
unrestr_cm <- confusionMatrix(as.factor(unrestr_preds),
wwines[-train_indices,]$quality,
positive = "low"
)
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
#restricted model performance
restr_probs <- predict(lr_fit_res,  wwines[-train_indices,], type="response")
restr_preds <- ifelse(restr_probs > 0.355,"low","high")
restr_cm <- confusionMatrix(factor(restr_preds,levels=c("low","high")),
wwines[-train_indices,]$quality,
positive = "low"
)
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])
data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance),
"improvement" = unlist(restr_performance)-unlist(unrestr_performance))
#unrestricted model performance
unrestr_probs <- predict(lr_fit,  wwines[-train_indices,], type="response")
unrestr_preds <- ifelse(unrestr_probs > 0.355,"low","high")
unrestr_cm <- confusionMatrix(as.factor(unrestr_preds),
wwines[-train_indices,]$quality,
positive = "low"
)
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
#restricted model performance
restr_probs <- predict(lr_fit_res,  wwines[-train_indices,], type="response")
restr_preds <- ifelse(restr_probs > 0.355,"low","high")
restr_cm <- confusionMatrix(factor(restr_preds,levels=c("high","low")),
wwines[-train_indices,]$quality,
positive = "low"
)
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])
data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance),
"improvement" = unlist(restr_performance)-unlist(unrestr_performance))
#unrestricted model performance
unrestr_probs <- predict(lr_fit,  wwines[-train_indices,], type="response")
unrestr_preds <- ifelse(unrestr_probs > 0.355,"low","high")
unrestr_cm <- confusionMatrix(as.factor(unrestr_preds),
wwines[-train_indices,]$quality,
positive = "low"
)
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
#restricted model performance
restr_probs <- predict(lr_fit_res,  wwines[-train_indices,], type="response")
restr_preds <- ifelse(restr_probs < 0.355,"low","high")
restr_cm <- confusionMatrix(factor(restr_preds,levels=c("high","low")),
wwines[-train_indices,]$quality,
positive = "low"
)
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])
data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance),
"improvement" = unlist(restr_performance)-unlist(unrestr_performance))
