---
title: "wine_quality"
author: "LJSSE"
date: "19/5/2021"
output: html_document
---

### Importing packages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(ggplot2)
library(dplyr)
library(reshape2)
library(mlr3learners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
```

################################################################################
## DATA

The dataset **white_wines.csv** stores data about physicochemical properties of many wines which come from the north-west region, named Minho, of Portugal.

Each row refers to a wine and each column contains one of the physicochemical properties with respect to wines. Here we list the properties:

- fixed acidity (FA: g(tartaric acid)/dm^3)
- volatile acidity (VA: g(acetic acid)/dm^3)
- citric acid (CA: g/dm^3)
- residual sugar (RS: g/dm^3)
- chlorides (CH: g(sodium chloride)/dm^3)
- free sulfur dioxide (FSD: mg/dm^3)
- total sulfur dioxide (TSD: mg/dm^3)
- density (DE: g/dm^3)
- pH
- sulphates (SU: g(potassium sulphate)/dm^3)
- alcohol (AL: %vol)
- quality (from 0 to 10)

Our aim will be to classify white wines according to their quality, labeling them as "low" if their quality score is from 0 to 5, and as "high" if the score is between 6 and 10.


## Loading the dataset

```{r, message=FALSE, warning = FALSE}
wwines <- read.csv("winequality-white.csv", sep=";")
print("White wines")
str(wwines)
```

Now we inspect the variables we have and we transform them if necessary.

## Defining the target: high and low quality

```{r, message=FALSE, warning = FALSE}
wwines$quality <- as.factor(case_when(wwines$quality <= 5 ~ "low",
                                      wwines$quality > 5 ~ "high"))
wwines$quality <- factor(wwines$quality, levels = c("low","high"))
table(wwines$quality)

```


## Data description

### Heatmap
```{r}
#compute on all features (without output variable which is at index 12)
cors <- cor(wwines[-12])
corrplot(cors, type = "upper", 
         tl.col = "black", tl.srt = 45)

```
We have to worry about two correlations: residual.sugar with density and density with alcohol.
We decide to remove density, as this will solve both the correlation issues, and we can see from the histograms below that the distribution of density does not depend on quality, while the shape of the residual.sugar and alcohol curves changes for low and high quality wines.

### Histograms
```{r}
h1 <- ggplot(wwines, aes(residual.sugar, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h2 <- ggplot(wwines, aes(alcohol, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h3 <- ggplot(wwines, aes(density, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

grid.arrange(h1,h2,h3, ncol=1)

```

```{r}
wwines$density <- NULL
```

### Correlation with target variable
```{r}
library(ltm)
for (i in 1:length(wwines)) {
  print(paste(colnames(wwines)[i],biserial.cor(wwines[,i],wwines$quality),sep = " "))
}
```


### Boxplots
```{r, message=FALSE, warning = FALSE}
summary(wwines)
boxplot(wwines$fixed.acidity ~ wwines$quality)
boxplot(wwines$volatile.acidity ~ wwines$quality)
boxplot(wwines$citric.acid ~ wwines$quality)
boxplot(wwines$residual.sugar ~ wwines$quality)
boxplot(wwines$chlorides ~ wwines$quality)
boxplot(wwines$free.sulfur.dioxide ~ wwines$quality)
boxplot(wwines$total.sulfur.dioxide ~ wwines$quality)
boxplot(wwines$density ~ wwines$quality)
boxplot(wwines$pH ~ wwines$quality)
boxplot(wwines$sulphates ~ wwines$quality)
boxplot(wwines$alcohol ~ wwines$quality)

```

Thanks to these plots we can suspect which variables will be significant to perform out regression. Here none of the variables seems to be too different comparing the two subsets (high and low quality).




###########################################################################

# LOGISTIC REGRESSION 

## Divide the dataset in training and test

```{r}
set.seed(42)
train_indices<- createDataPartition(wwines$quality,p=0.7,list=FALSE)
```

## Complete model: all variables are included

Estimated coefficients and Odds Ratios.
The baseline for the regression is "high", because positive: "low" 

```{r, message=FALSE, warning=FALSE}
lr_fit <- glm(quality ~., data =  wwines[train_indices,],
          family=binomial(link='logit'))

# coefficients
summary(lr_fit)

# odds ratios
exp(cbind(OR = coef(lr_fit), confint(lr_fit)))
```
Here we find these variable which are significant at 99% confidence level: ◊check values
- volatile.acidity: a unitary increase in volatile acidity will lead to a 99.84% decrease in the odds of having a low quality wine.
- residual.sugar: a unitary increase in volatile acidity will lead to a 15.77% increase in the odds of having a low quality wine.
- sulphates: a unitary increase in volatile acidity will lead to a 407.37% increase in the odds of having a low quality wine.
- alcohol: a unitary increase in volatile acidity will lead to a 131.11% increase in the odds of having a low quality wine.
- free.sulfur.dioxide: a unitary increase in volatile acidity will lead to a 1.23% increase in the odds of having a low quality wine.

We find these variable which are significant at 95% confidence level:
- (Intercept): ◊this value corresponds to the increase in the odds of having a low quality wine when all the explanatory variables are set at 0.
- density: a unitary increase in volatile acidity will lead to a 100% decrease in the odds of having a low quality wine.

We find this variable which is significant at 90% confidence level:
- pH: a unitary increase in volatile acidity will lead to a 104.46% increase in the odds of having a low quality wine.
- total.sulfur.dioxide: a unitary increase in volatile acidity will lead to a 0.24866% decrease in the odds of having a low quality wine.


### Finding the optimal threshold for discriminating high quality wines from low quality wines

```{r}
#here we use mlr3 package
#install_learners('classif.log_reg')
#mlr_learners$get("classif.log_reg")

#defining our task: classification on wines with quality as response variable and low quality as baseline
task_train = TaskClassif$new(id = "wines",  wwines[train_indices,], target = "quality", positive = "low")
#defining the learner which will perform the training procedure on our model
learner_train = lrn("classif.log_reg", predict_type = "prob")
#defining how we want to perform cross validation (setting number of folds to 5)
cv5 = rsmp("cv", folds = 5)
#we want to tweak the threshold of our classification: we analyze values from 0.3 to 0.6 with step 0.01
thresholds <- seq(0.3,0.6,0.01)

#collecting performances with different thresholds
accuracy <- list()
sensitivity <- list()
specificity <- list()
auroc <- list()
i <- 1
for (thresh in thresholds) {
    res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
    #combined prediction of all individual resampling iterations
    prediction <- res_cv$prediction()
    prediction$set_threshold(thresh)
    #scores are combined as well from all individual resampling iterations
    accuracy[i] <- prediction$score(measures = msr("classif.acc"), task = task_train)
    sensitivity[i] <- prediction$score(measures = msr("classif.sensitivity"), task = task_train)
    specificity[i] <- prediction$score(measures = msr("classif.specificity"), task = task_train)
    auroc[i] <- prediction$score(measures = msr("classif.auc"), task = task_train)
    i<-i+1
}

#collecting scores
measures <- data.frame(thresholds,
                       "accuracy" =unlist(accuracy), 
                       "sensitivity" =unlist(sensitivity),
                       "specificity" = unlist(specificity),
                       "auroc" = unlist(auroc))
  
#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) + 
  geom_line() +
  geom_vline(xintercept = mean(thresholds[intersection_indices]),linetype = "dotted") +
  geom_label(aes(x = 0.37, y = 0.5, label = "0.355"))

```

Optimal threshold is 0.355.
Since the choice of the threshold depends on the aim of the analysis, we decided to use the level which corresponds to the intersection between accuracy, specificity, and sensitivity, as our aim is to build a predictor which would havee the best behaviour on different data.◊

### Complete model test performance 
```{r}
# define new task for  wwines[-train_indices,]
task_test = TaskClassif$new(id = "wines_test",  wwines[-train_indices,], target = "quality", positive = "low")
# train on train task
learner_train$train(task_train)
# predict on test task
pred <- learner_train$predict(task_test)
# set optimal threshold
pred$set_threshold(mean(thresholds[intersection_indices]))
# performance
cm <- list("confusion" = pred$confusion,
          "accuracy" = pred$score(measures = msr("classif.acc")),
          "sensitivity"=pred$score(measures = msr("classif.sensitivity")),
          "specificity"=pred$score(measures = msr("classif.specificity")))
cm

```

## Model selection
◊ get a better viz
### vif
```{r}
a <- vif(lr_fit)
#b <- data.frame(a,colnames(wwines)[-12])  %>% mutate(sqrt = sqrt(a)) %>% filter(sqrt > 2)
sqrt(vif(lr_fit))>2 # residual.sugar, density, alcohol
```

Variance inflation factor measures how much the behavior (variance) of an independent variable is influenced, or inflated, by its interaction/correlation with the other independent variables.

Residual sugar, density and alcohol are the most influenced by their interactions with other independent variables.


### Forward Stepwise method
```{r}
regfit.fwd=regsubsets(quality~.,data=wwines,method="forward", nvmax=13)
summary.fwd <- summary(regfit.fwd)

plot(summary.fwd$bic,xlab="Number of Variables",ylab="bic")
points(which.min(summary.fwd$bic),summary.fwd$bic[which.min(summary.fwd$bic)],pch=20,col="red")
```
Forward stepwise steps:
1. inclusion of alcohol
2. inclusion of volatile.acidity
3. inclusion of residual.sugar
4. inclusion of sulphates
5. inclusion of density
6. inclusion of pH
7. inclusion of free.sulfur.dioxide
8. inclusion of total.sulfur.dioxide
9. inclusion of fixed.acidity
10. inclusion of citric.acid
11. inclusion of chlorides

```{r}
plot(regfit.fwd,scale="bic")
grid(nx=11,ny=10,col="red",lty = "solid")
abline(h=10, col="yellow", lwd = 10)
text(10.5,9, "Optimal model", col="yellow",cex = 0.8)
```


## Restricted model: only 6 variables

Only including: fixed.acidity, volatile.acidity, residual.sugar, free.sulfur.dioxide, sulphates, alcohol.
```{r}
lr_fit_res <- glm(quality ~ fixed.acidity+volatile.acidity+residual.sugar+free.sulfur.dioxide+sulphates+alcohol, data =  wwines[train_indices,],
          family=binomial(link='logit'))

# coefficients
summary(lr_fit_res)

# odds ratios
exp(cbind(OR = coef(lr_fit_res), confint(lr_fit_res)))
```
Here we find these variable which are significant at 99% confidence level: ◊check values
- (Intercept)
- volatile.acidity: a unitary increase in volatile acidity will lead to a 99.86127% decrease in the odds of having a low quality wine.
- residual.sugar: a unitary increase in volatile acidity will lead to a 17.2293% increase in the odds of having a low quality wine.
- free.sulfur.dioxide: a unitary increase in volatile acidity will lead to a 0.9025% increase in the odds of having a low quality wine.
- density: a unitary increase in volatile acidity will lead to a 100% decrease in the odds of having a low quality wine.
- sulphates: a unitary increase in volatile acidity will lead to a 377.0417% increase in the odds of having a low quality wine.
- alcohol: a unitary increase in volatile acidity will lead to a 124.092% increase in the odds of having a low quality wine.

We find these variable which are significant at 90% confidence level:
- pH: a unitary increase in volatile acidity will lead to a 125.6878% increase in the odds of having a low quality wine.

## Test performances of restricted and unrestricted models
```{r, warning=FALSE}
#unrestricted model performance
unrestr_probs <- predict(lr_fit,  wwines[-train_indices,], type="response")
unrestr_preds <- ifelse(unrestr_probs > 0.355,"low","high")
unrestr_cm <- confusionMatrix(as.factor(unrestr_preds),
                         wwines[-train_indices,]$quality,
                        positive = "low"
                        )
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])

#restricted model performance
restr_probs <- predict(lr_fit_res,  wwines[-train_indices,], type="response")
restr_preds <- ifelse(restr_probs > 0.355,"low","high")
restr_cm <- confusionMatrix(as.factor(restr_preds),
                         wwines[-train_indices,]$quality,
                        positive = "low"
                        )
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])


data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance), 
           "improvement" = unlist(restr_performance)-unlist(unrestr_performance))
```
There is no improvement in using the restricted model rather than the unrestricted.

## Detecting outliers (first)
```{r, warning=FALSE}
#see Cook's distance
influenceIndexPlot(lr_fit, id.n=10)  # 2782, 4481
#using dffits
df_fits<-dffits(lr_fit)
threshold<-(2*(11+1)/sqrt(3429-11-1))  
which(abs(df_fits)>threshold) # 2782, 1956
outliers <- c(1956,2782,4481)
 wwines[train_indices,]["outlier"] <- ifelse(rownames( wwines[train_indices,]) %in% outliers,0,1)
residualPlots(lr_fit, id = TRUE,  col = as.factor( wwines[train_indices,]$outlier))
```
From the first plot we identify as outliers instances number 2782 and 4481 (see Cook's distance plot). These instances are outliers for the independent variables distributions.

2782 and 1956 are outliers for the distribution of the response variable (dffits).

## Detecting outliers using unrestricted model (second)
```{r, warning=FALSE}
library(broom)
#dffits threshold
threshold<-(2*(ncol( wwines[train_indices,])+1)/sqrt(nrow( wwines[train_indices,])-ncol( wwines[train_indices,])-1))  
d <- dffits(lr_fit)
#another way to get cook's distance
#cooks distance threshold: https://www.scikit-yb.org/en/latest/api/regressor/influence.html#:~:text=Because%20of%20this%2C%20Cook's%20Distance,that%20is%20above%20that%20threshold.
plot(lr_fit, which = 4, id.n = 3)
outliers <- 
  augment(lr_fit) %>%
  mutate(df = unname(dffits(lr_fit)))%>%
  filter(.cooksd > 4/nrow( wwines[train_indices,]) | abs(df)>threshold) %>%
  select(.rownames)

 wwines[train_indices,]["outlier"] <- ifelse(rownames( wwines[train_indices,]) %in% outliers$.rownames,0,1)
residualPlots(lr_fit, id = TRUE, col = as.factor( wwines[train_indices,]$outlier), ask = FALSE)
```


## Outliers free model (first)
```{r}
 wwines[train_indices,]$outlier<- NULL
 wwines[train_indices,]_outlierfree <-  wwines[train_indices,][-outliers,]
lr_fit_outlierfree <- glm(quality ~., data =  wwines[train_indices,]_outlierfree,
                          family=binomial(link='logit'))

# coefficients
summary(lr_fit_outlierfree)

# odds ratios
exp(cbind(OR = coef(lr_fit_outlierfree), confint(lr_fit_outlierfree)))
```

## Outliers free model (second)
```{r}
 wwines[train_indices,]$outlier<- NULL
 wwines[train_indices,]_outlierfree <-  wwines[train_indices,][-as.numeric(outliers$.rownames),]
lr_fit_outlierfree <- glm(quality ~., data =  wwines[train_indices,]_outlierfree,
                          family=binomial(link='logit'))

# coefficients
summary(lr_fit_outlierfree)

# odds ratios
exp(cbind(OR = coef(lr_fit_outlierfree), confint(lr_fit_outlierfree)))
```

## With and without outliers

### Estimated coefficients
```{r}
outliers <- data.frame(summary(lr_fit)$coefficient)
no_outliers<- data.frame(summary(lr_fit_outlierfree)$coefficient)

variable<-c("(constant)",colnames(wwines)[-12])

data.frame(variable,outliers$Estimate,no_outliers$Estimate, "difference"=outliers$Estimate-no_outliers$Estimate)
```

### Performance on test
```{r}
# outliers free
lr_prob_of <- predict(lr_fit_outlierfree,  wwines[-train_indices,], type="response")
lr_pred_of <- ifelse(lr_prob_of > 0.355,"low","high")
cm_of <- confusionMatrix(as.factor(lr_pred_of),
                         wwines[-train_indices,]$quality,
                        positive = "low"
                        )

out_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
outfree_performance <- list(cm_of$overall[1], cm_of$byClass[1], cm_of$byClass[2])

data.frame("outliers_performance" = unlist(out_performance),"outliers_free_performance" = unlist(outfree_performance), 
           "improvement" = unlist(outfree_performance)-unlist(out_performance))
```

# Outliers free ROC Curve

```{r, message=FALSE, warning=FALSE}
test_roc = roc( wwines[-train_indices,]$quality ~ lr_prob_of, plot = TRUE, print.auc = TRUE)

```

################################################################################
# BAGGING 
```{r}
n <- seq(nrow( wwines[train_indices,]))
set.seed(1)
S1 <- sample(n, nrow( wwines[train_indices,]), replace = TRUE)
set.seed(2)
S2 <- sample(n, nrow( wwines[train_indices,]), replace = TRUE)
set.seed(3)
S3 <- sample(n, nrow( wwines[train_indices,]), replace = TRUE)
set.seed(4)
S4 <- sample(n, nrow( wwines[train_indices,]), replace = TRUE)
lr_b <- function(i){
      lr_f <- glm(quality ~ volatile.acidity+residual.sugar+free.sulfur.dioxide+density+pH+sulphates+alcohol, 
                  data =  wwines[train_indices,][i, ],family=binomial(link='logit'))
      lr_prob <- predict(lr_f, wwines[train_indices,][i,], type="response")
      lr_pred <- ifelse(lr_prob > 0.355,"low","high")
      cm <- confusionMatrix(
                as.factor(lr_pred),
                 wwines[train_indices,][i,]$quality,
                positive = "low"
                )
      a <- cm$overall[[1]]
      return(a)
}
aggregate = (lr_b(S1) +lr_b(S2) + lr_b(S3) + lr_b(S4))/4
round(aggregate,4)
```

################################################################################

# Relation with restricted model variables

volatile.acidity, residual.sugar, free.sulfur.dioxide, density, 
pH, sulphates, alcohol


```{r}
ggplot(wwines, aes(x = volatile.acidity, color = quality)) +
  geom_histogram() +
  labs(x = "volatile.acidity",
       title = "Wine quality by volatile.acidity")

ggplot(wwines, aes(x = residual.sugar, color = quality)) +
  geom_histogram() +
  labs(x = "residual.sugar",
       title = "Wine quality by residual.sugar")

ggplot(wwines, aes(x = free.sulfur.dioxide, color = quality)) +
  geom_histogram() +
  labs(x = "free.sulfur.dioxide",
       title = "Wine quality by free.sulfur.dioxide")

ggplot(wwines, aes(x = density, color = quality)) +
  geom_histogram() +
  labs(x = "density",
       title = "Wine quality by density")

```
