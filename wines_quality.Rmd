---
title: "wine_quality"
author: "LJSSE"
date: "19/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(ggplot2)
library(dplyr)
library(reshape2)
library(mlr3learners)
library(mlr3extralearners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
```

################################################################################
## DATA

The dataset **white_wines.csv** stores data about physicochemical properties of many wines which come from the north-west region, named Minho, of Portugal.

Each row refers to a wine and each column contains one of the physicochemical properties with respect to wines. Here we list the properties:

- fixed acidity (FA: g(tartaric acid)/dm^3)
- volatile acidity (VA: g(acetic acid)/dm^3)
- citric acid (CA: g/dm^3)
- residual sugar (RS: g/dm^3)
- chlorides (CH: g(sodium chloride)/dm^3)
- free sulfur dioxide (FSD: mg/dm^3)
- total sulfur dioxide (TSD: mg/dm^3)
- density (DE: g/dm^3)
- pH
- sulphates (SU: g(potassium sulphate)/dm^3)
- alcohol (AL: %vol)
- quality (from 0 to 10)

Our aim will be to classify white wines according to their quality, labelling them as "low" if 
their quality score is from 0 to 5, and as "high" if the score is between 6 and 10.


## Loading the dataset

```{r, message=FALSE, warning = FALSE}
wwines <- read.csv("winequality-white.csv", sep=";")
print("White wines")
str(wwines)
```

Now we inspect the variables we have and we transform them if necessary.

## Defining the target

```{r, message=FALSE, warning = FALSE}
wwines$quality <- as.factor(case_when(wwines$quality <= 5 ~ "low",
                                      wwines$quality > 5 ~ "high"))
wwines$quality <- factor(wwines$quality, levels = c("low","high"))
table(wwines$quality)

```


## Data description

### heatmap
```{r}
cors <- cor(wwines[-12])
corrplot(cors, type = "upper", 
         tl.col = "black", tl.srt = 45)
```

### boxplots
```{r, message=FALSE, warning = FALSE}
summary(wwines)
boxplot(wwines$fixed.acidity ~ wwines$quality)
boxplot(wwines$volatile.acidity ~ wwines$quality)
boxplot(wwines$citric.acid ~ wwines$quality)
boxplot(wwines$residual.sugar ~ wwines$quality)
boxplot(wwines$chlorides ~ wwines$quality)
boxplot(wwines$free.sulfur.dioxide ~ wwines$quality)
boxplot(wwines$total.sulfur.dioxide ~ wwines$quality)
boxplot(wwines$density ~ wwines$quality)
boxplot(wwines$pH ~ wwines$quality)
boxplot(wwines$sulphates ~ wwines$quality)
boxplot(wwines$alcohol ~ wwines$quality)

```

### histograms
```{r}
h1 <- ggplot(wwines, aes(fixed.acidity, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h2 <- ggplot(wwines, aes(volatile.acidity, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h3 <- ggplot(wwines, aes(citric.acid, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h4 <- ggplot(wwines, aes(residual.sugar, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

grid.arrange(h1,h2,h3,h4, ncol=1)

```

```{r}
h1 <- ggplot(wwines, aes(chlorides, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h2 <- ggplot(wwines, aes(free.sulfur.dioxide, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h3 <- ggplot(wwines, aes(total.sulfur.dioxide, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h4 <- ggplot(wwines, aes(density, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

grid.arrange(h1,h2,h3,h4, ncol=1)
```

```{r}
h1 <- ggplot(wwines, aes(pH, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h2 <- ggplot(wwines, aes(sulphates, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h3 <- ggplot(wwines, aes(alcohol, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)


grid.arrange(h1,h2,h3, ncol=1)
```

###########################################################################

# LOGISTIC REGRESSION 

## Divide the dataset in training and test

```{r}
set.seed(42)
split_train_test <- createDataPartition(wwines$quality,p=0.7,list=FALSE)
dtrain<- wwines[split_train_test,]
dtest<-  wwines[-split_train_test,]
```

## Complete model

Estimated coefficients and Odds Ratios

```{r, message=FALSE, warning=FALSE}
lr_fit <- glm(quality ~., data = dtrain,
          family=binomial(link='logit'))

# coefficients
summary(lr_fit)

# odds ratios
exp(cbind(OR = coef(lr_fit), confint(lr_fit)))
```

### Finding the optimal threshold

```{r}

install_learners('classif.log_reg')
mlr_learners$get("classif.log_reg")

task_train = TaskClassif$new(id = "wines", dtrain, target = "quality", positive = "low")
learner_train = lrn("classif.log_reg", predict_type = "prob")
cv5 = rsmp("cv", folds = 5)
thresholds <- seq(0.3,0.6,0.01)

accuracy <- list()
sensitivity <- list()
specificity <- list()
auroc <- list()
i <- 1
for (thresh in thresholds) {
    res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
    prediction <- res_cv$prediction()
    prediction$set_threshold(thresh)
    accuracy[i] <- prediction$score(measures = msr("classif.acc"), task = task_train)
    sensitivity[i] <- prediction$score(measures = msr("classif.sensitivity"), task = task_train)
    specificity[i] <- prediction$score(measures = msr("classif.specificity"), task = task_train)
    auroc[i] <- prediction$score(measures = msr("classif.auc"), task = task_train)
    i<-i+1
}

measures <- data.frame(thresholds,
                       "accuracy" =unlist(accuracy), 
                       "sensitivity" =unlist(sensitivity),
                       "specificity" = unlist(specificity),
                       "auroc" = unlist(auroc))
  
# What is the intersection?
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) + 
  geom_line() +
  geom_vline(xintercept = mean(thresholds[intersection_indices]),linetype = "dotted") +
  geom_label(aes(x = 0.37, y = 0.5, label = "0.355"))

```

### complete model test performance 
```{r}
# define new task for dtest
task_test = TaskClassif$new(id = "wines_test", dtest, target = "quality", positive = "low")
# train on train task
learner_train$train(task_train)
# predict on test task
pred <- learner_train$predict(task_test)
# set optimal threshold
pred$set_threshold(0.355)
# performance
cm <- list("confusion" = pred$confusion,
          "accuracy" = pred$score(measures = msr("classif.acc")),
          "sensitivity"=pred$score(measures = msr("classif.sensitivity")),
          "specificity"=pred$score(measures = msr("classif.specificity")))
cm


```

## Model selection

### vif
```{r}
vif(lr_fit)
sqrt(vif(lr_fit))>2 # residual.sugar, density, alcohol
```

### Best Subset regression
```{r}
library(leaps)
regfit.full=regsubsets(quality~.,data=wwines, nvmax=13)
reg.summary=summary(regfit.full)
names(reg.summary)

plot(reg.summary$bic,xlab="Number of Variables",ylab="bic")
points(which.min(reg.summary$bic),reg.summary$bic[which.min(reg.summary$bic)],pch=20,col="red")

```

```{r}
plot(regfit.full,scale="bic")
grid(nx=12,ny=11,col="red",lty = "solid")
```

### Forward Stepwise
```{r}
regfit.fwd=regsubsets(quality~.,data=wwines,method="forward", nvmax=13)
summary(regfit.fwd)

```

```{r}
plot(regfit.fwd,scale="bic")
grid(nx=12,ny=11,col="red",lty = "solid")
```


## Restricted model
Only including: volatile.acidity, residual.sugar, free.sulfur.dioxide, density, 
pH, sulphates, alcohol
```{r}
lr_fit_sig <- glm(quality ~ volatile.acidity+residual.sugar+free.sulfur.dioxide+density+pH+sulphates+alcohol, data = dtrain,
          family=binomial(link='logit'))

# coefficients
summary(lr_fit_sig)

# odds ratios
exp(cbind(OR = coef(lr_fit_sig), confint(lr_fit_sig)))
```


## DETECTING OUTLIERS
```{r, warning=FALSE}
influenceIndexPlot(lr_fit, id.n=10)  # 2782, 4746
a<-dffits(lr_fit) 
threshold<-(2*(11+1)/sqrt(3429-11-1))  
which(abs(a)>threshold) # 2782, 1971
outliers <- c(1971, 2782, 4746,4481,3737)
dtrain["outlier"] <- ifelse(rownames(dtrain) %in% outliers,0,1)
residualPlots(lr_fit, id = TRUE,  col = as.factor(dtrain$outlier))
```


## Outliers free model
```{r}
dtrain$outlier<- NULL
dtrain_outlierfree <- dtrain[-outliers,]
lr_fit_outlierfree <- glm(quality ~., data = dtrain_outlierfree,
                          family=binomial(link='logit'))

# coefficients
summary(lr_fit_outlierfree)

# odds ratios
exp(cbind(OR = coef(lr_fit_outlierfree), confint(lr_fit_outlierfree)))
```


## With and without outliers

### Estimated coefficients
```{r}
outliers <- data.frame(summary(lr_fit)$coefficient)
no_outliers<- data.frame(summary(lr_fit_outlierfree)$coefficient)

variable<-c("(constant)",colnames(wwines)[-12])

data.frame(variable,outliers$Estimate,no_outliers$Estimate, "difference"=outliers$Estimate-no_outliers$Estimate)
```

### Performance on test
```{r}
# outliers free
lr_prob_of <- predict(lr_fit_outlierfree, dtest, type="response")
lr_pred_of <- ifelse(lr_prob_of > 0.355,"low","high")
cm_of <- confusionMatrix(as.factor(lr_pred_of),
                        dtest$quality,
                        positive = "low"
                        )

out_performance <- list(cm$overall[1], cm$byClass[1], cm$byClass[2])
outfree_performance <- list(cm_of$overall[1], cm_of$byClass[1], cm_of$byClass[2])

data.frame("outliers_performance" = unlist(out_performance),"outliers_free_performance" = unlist(outfree_performance), 
           "improvement" = unlist(outfree_performance)-unlist(out_performance))
```

# Outliers free ROC Curve

```{r, message=FALSE, warning=FALSE}
test_roc = roc(dtest$quality ~ lr_prob_of, plot = TRUE, print.auc = TRUE)

```

################################################################################
# BAGGING 
```{r}
n <- seq(nrow(dtrain))
set.seed(1)
S1 <- sample(n, nrow(dtrain), replace = TRUE)
set.seed(2)
S2 <- sample(n, nrow(dtrain), replace = TRUE)
set.seed(3)
S3 <- sample(n, nrow(dtrain), replace = TRUE)
set.seed(4)
S4 <- sample(n, nrow(dtrain), replace = TRUE)
lr_b <- function(i){
      lr_f <- glm(quality ~ volatile.acidity+residual.sugar+free.sulfur.dioxide+density+pH+sulphates+alcohol, 
                  data = dtrain[i, ],family=binomial(link='logit'))
      lr_prob <- predict(lr_f,dtest[i,], type="response")
      lr_pred <- ifelse(lr_prob > 0.355,"low","high")
      cm <- confusionMatrix(
                as.factor(lr_pred),
                dtest[i,]$quality,
                positive = "low"
                )
      a <- cm$overall[[1]]
      return(a)
}
aggregate = (lr_b(S1) +lr_b(S2) + lr_b(S3) + lr_b(S4))/4
round(aggregate,4)
```

################################################################################

# Relation with restricted model variables

volatile.acidity, residual.sugar, free.sulfur.dioxide, density, 
pH, sulphates, alcohol


```{r}
ggplot(wwines, aes(x = volatile.acidity, color = quality)) +
  geom_histogram() +
  labs(x = "volatile.acidity",
       title = "Wine quality by volatile.acidity")

ggplot(wwines, aes(x = residual.sugar, color = quality)) +
  geom_histogram() +
  labs(x = "residual.sugar",
       title = "Wine quality by residual.sugar")

ggplot(wwines, aes(x = free.sulfur.dioxide, color = quality)) +
  geom_histogram() +
  labs(x = "free.sulfur.dioxide",
       title = "Wine quality by free.sulfur.dioxide")

ggplot(wwines, aes(x = density, color = quality)) +
  geom_histogram() +
  labs(x = "density",
       title = "Wine quality by density")

```
