---
title: "White wines' quality classification with Logistic Regression"
author: "LJSSE"
date: "19/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(ggplot2)
library(dplyr)
library(reshape2)
library(mlr3learners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
library(broom)
library(ltm)
library(MASS)
library(leaps)
```

![](wines.jpeg)

# Introduction

The dataset **white_wines.csv** stores data about physicochemical properties of many white wines coming from the north-west region of Portugal, called Minho.

Each row refers to a wine, while each column contains a physicochemical property.\
Here we list the properties and their unit of measure:

* fixed acidity (FA: g(tartaric acid)/dm^3)
* volatile acidity (VA: g(acetic acid)/dm^3)
* citric acid (CA: g/dm^3)
* residual sugar (RS: g/dm^3)
* chlorides (CH: g(sodium chloride)/dm^3)
* free sulfur dioxide (FSD: mg/dm^3)
* total sulfur dioxide (TSD: mg/dm^3)
* density (DE: g/dm^3)
* pH
* sulphates (SU: g(potassium sulphate)/dm^3)
* alcohol (AL: %vol)
* *target*: quality (from 0 to 10)

The aim of this project will be to classify white wines according to their quality, labeling them as "low" if their quality score is from 0 to 5, and as "high" if the score is between 6 and 10.

____

# The data

First, we nedd to inspect the variables and make the suitable transformations, if necessary.

```{r, message=FALSE, warning = FALSE}
wwines <- read.csv("winequality-white.csv", sep=";")
print("White wines")
str(wwines)
```

All the features are numeric, and are already stored in the right format.\

## Defining the target

We re-encode "quality" as a factor with two levels, in order to make the target binary.\
As we can see the two categories are unbalanced, with the data containing a higher proportion of high quality wines.

```{r, message=FALSE, warning = FALSE}
wwines$quality <- as.factor(case_when(wwines$quality <= 5 ~ "low",
                                      wwines$quality > 5 ~ "high"))
table(wwines$quality)
```

In order to have a unique identifier for each wine, we also create an "id" variable.
```{r}
wwines["id"] <- rownames(wwines)
```

___

# Data description

Before proceeding with our analysis, we perform some descriptive statistics in order to both understand the relations within the variables, and to visualize their distributions. This will help us to get a first idea of the relevance of the features for our classification task.

### Heatmap

First we plot the correlation matrix, in order to detect any strong relation among features which could lead to multicollinearity problems.
```{r}
cors <- cor(wwines[-c(12,13)])
corrplot(cors, type = "upper",tl.col = "black", tl.srt = 45)
```
This plot highlights a strong correlation between density and two variables: residual.sugar and alcohol.

### Frequency polygons

To get an understanding of which variable should be excluded from the regression, we plot the histograms of the highly correlated variables identified above.
```{r}
residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

density.plot <- ggplot(wwines, aes(density, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)

```
We decide to remove density.\
In fact, we can observe that for each value of density there is no difference in the distribution of quality, once we account for the numerosity of the two quality levels. On the other hand, the modal values for the other two variables differ for low and high quality wines, thus adding more useful information for the purpose of classification.

```{r}
wwines$density <- NULL
```


### Correlation with the target 

It is also useful to compute the correlations between the covariates and the target, in order to see if there are any particularly powerful features that would make the task too trivial.\
In order to do so we exploit *biserial correlation*, which allows to compute a correlation index between a numerical and a binary variable.
```{r, warning=FALSE, message=FALSE}
wwines$quality <- factor(wwines$quality, levels = c("high","low"))
bicorrs <- sapply(wwines[,-c(11,12)], function(x) round(biserial.cor(x,wwines$quality),2))
data.frame(bicorrs) %>% arrange(abs(bicorrs))
```
Here the correlations are displayed in ascending absolute value, and we can observe that the variable showing the highest correlation with wines quality is alcohol, but it is not too strong (38%).
The sign is positive, meaning that, in our data, wines with a greater alcohol percentage are likely to be of higher quality.

### Boxplots

We now look at the distribution of all the explanatory variables, in order to both visualize those values which are far from the bulk of the data, and to get an understanding of the variables displaying the most divergent median values among low and high quality wines.

```{r, message=FALSE, warning = FALSE}
# melt the dataset
wwines.m <- melt(wwines[,-12], id.var = "quality")

# wrapped boxplots
ggplot(data = wwines.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=quality)) +
facet_wrap( ~ variable, scales="free")
```

We can observe the following:
  * The features displaying the highest median difference between high and low quality wines are "volatile.acidity", "total.sulfur.dioxide", "pH", "alcohol." Theese variables are candidates to be the significant ones in the regression framework.
  * There are some outlying values. We will deal with them in the section below.

___

## Data splitting

In order to train and validate our model on different sets, we split the data, keeping 30% of it to carry out performance evaluation.

```{r}
set.seed(42)
train_indices<- createDataPartition(wwines$quality,p=0.7,list=FALSE)
dtrain <- wwines[train_indices,]
dtest <- wwines[-train_indices,]
```

____

____

# VARIABLES SELECTION

## Forward Stepwise Method

To find the main factors determining wine quality and to remove the unrelated ones, we use forward stepwsie method to do variable selection.\
The best subset of the model can be found by starting from a null model with no predictors, then picking the best p (p is the number of predictors in the full model) models which have exactly 1, 2, ..., p predictors, and finally selecting the model with the lowest Bayesian information criterion (BIC) value.\
In our case, the inclusion of the variables follows the order of alcohol, volatile.acidity, residual.sugar, sulphates, fixed.acidity, free.sulfur.dioxide, total.sulfur.dioxide, pH, chlorides and citric.acid.\
When the model contains 6 variables, it has lowest BIC value. So, the variables of the optimal model are the six first ones.

```{r}
regfit.fwd=regsubsets(quality~.,data=wwines[,-12],method="forward", nvmax=13)
summary.fwd <- summary(regfit.fwd)

plot(summary.fwd$bic,xlab="Number of Variables",ylab="bic")
points(which.min(summary.fwd$bic),summary.fwd$bic[which.min(summary.fwd$bic)],pch=20,col="red")
```

Optimal number of variables: 6.


We plot the results in order to visualize which variables have been selected with BIC in each model, and we highlight the optimal model with a yellow line. 

```{r}
plot(regfit.fwd,scale="bic")
grid(nx=11,ny=10,col="red",lty = "solid")
abline(h=10, col="yellow", lwd = 10)
text(10.5,9, "Optimal model", col="yellow",cex = 0.8)
```

___

# LOGISTIC REGRESSION

Logistic Regression is now performed on the restricted model with the six selected variables:\
fixed.acidity, volatile.acidity, residual.sugar, free.sulfur.dioxide, sulphates, alcohol.

With `factor()` we relevel the variable "quality", in order to set "low" as the baseline for the logistic regression.

```{r}
# relevel
wwines$quality <- factor(wwines$quality, levels = c("low","high"))

# remove unused variables
dtrain.res <- dtrain[,c(1,2,4,6,9,10,11,12)]
dtest.res <- dtest[,c(1,2,4,6,9,10,11,12)]

# train the model
lr_fit_res <- glm(quality ~., data =  dtrain.res[,-8],
          family=binomial(link='logit'))

# coefficients
summary(lr_fit_res)

# odds ratios
exp(cbind(OR = coef(lr_fit_res), confint(lr_fit_res)))
```

The following variables are those significant at a 99% confidence level:\
  * "Intercept": the value 440232.6% corresponds to the increase in the odds of having a high quality wine when all the explanatory variables are set to 0.
  * "fixed.acidity": a unitary increase (1 g/dm^3) in fixed acidity will lead to a 23.13% increase in the odds of having a high quality wine.
  * "volatile.acidity": a unitary increase (1 g/dm^3) in volatile acidity will lead to a 102318.1% increase in the odds of having a high quality wine.
  * "residual.sugar": a unitary increase (1 g/dm^3) in residual sugar will lead to a 6.76% decrease in the odds of having a high quality wine.
  * "free.sulfur.dioxide": a unitary increase (1 mg/dm^3) in free sulfur dioxide will lead to a 0.62% decrease in the odds of having a high quality wine.
  * "alcohol": a unitary increase (percentage) in alcohol will lead to a 66.04% decrease in the odds of having a high quality wine.

The following variable is significant at a 95% confidence level:\
  * "sulphates": a unitary increase (1 g/dm^3) in sulphates will lead to a 73.22% decrease in the odds of having a high quality wine.

___

## Optimal threshold

In order to get the best possible performance, we perform threshold optimization exploiting the `mlr3 package`. 

```{r,results='hide'}
# task: classification of training data
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
# type of learner: logistic regression
learner_train = lrn("classif.log_reg", predict_type = "prob")
# resampling method: cross validation
cv5 = rsmp("cv", folds = 5)
# defing the search space
thresholds <- seq(0.5,0.8,0.01)

#collecting cross-validated performances for different thresholds
measures_list <- rep(list(list()), 3)

for (thresh in thresholds) {
    res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
    #combined prediction of all individual resampling iterations
    prediction <- res_cv$prediction()
    prediction$set_threshold(thresh)
    #computing the scores
    scores <- prediction$score(measures = c(msr("classif.acc"),msr("classif.sensitivity"),msr("classif.specificity")))
    coefficients <- (unname(scores))
    measures_list <- mapply(append, measures_list, coefficients, SIMPLIFY = FALSE)
}

```

Since the choice of the threshold strictly depends on the aim of the analysis, which in our case is to build a predictor which would have the best behaviour in classifying new different data, we look for a threshold which could guarantee an equilibrium between the two types of errors.

```{r}
measures <- data.frame(thresholds,
                       "accuracy" = unlist(measures_list[[1]]), 
                       "sensitivity" = unlist(measures_list[[2]]),
                       "specificity" = unlist(measures_list[[3]]))


#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
th <- mean(thresholds[intersection_indices]) 

melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) + 
  geom_line() +
  geom_vline(xintercept = th,linetype = "dotted") +
  geom_label(aes(x = 0.67, y = 0.5, label = as.character(mean(thresholds[intersection_indices]))))
```

The optimal threshold for our aim is 0.65.\
This threshold meets our expectations, as the equilibrium point corresponds to a value higher than 0.5. \
This higher threshold compensates for the higher a priori probability of labelling a wine as high quality (low: 1640, high: 3258).

___

## Model's test performance 

To evaluate the performance of the estimated model, we use the Confusion Matrix to compute the accuracy, sensitivity and sensibility scores.

```{r}
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
task_test = TaskClassif$new(id = "wines_test",  dtest.res[,-8], target = "quality", positive = "high")

learner = lrn("classif.log_reg", predict_type = "prob")
cv5 = rsmp("cv", folds = 5)

learner$train(task_train) # training on train set
base_pred <- learner$predict(task_test) # predicting on test set
base_pred$set_threshold(th)

# performance
cm_base <- list("confusion" = base_pred$confusion,
          "accuracy" = base_pred$score(measures = msr("classif.acc")),
          "sensitivity"=base_pred$score(measures = msr("classif.sensitivity")),
          "specificity"=base_pred$score(measures = msr("classif.specificity")))
cm_base
```
We can state that:
  * the model predicts wines quality with 71% accuracy.
  * 71.55% of wines are correctly identified as high quality wines.
  * 69.92% of wines are correctly identified as low quality wines.

___

### Full/Restricted Performance comparison 

In order to see how the model's performance is affected by the dimensionality reduction, we compute the predictions for both the restricted and the full model and we compare the values of accuracy, sensitivity, and specificity.
By removing 5 out of 11 explanatory variables we are trying to avoid overfitting, thus creating a more stable model with a lower variance; of course, we expect the performance to be slightly worse, as we are introducing some bias.


```{r, warning=FALSE}
#running the full model
lr_fit <- glm(quality ~., data =  dtrain[,-12],
          family=binomial(link='logit'))

#full model performance
unrestr_probs <- predict(lr_fit,  dtest[,-12], type="response")
unrestr_preds <- ifelse(unrestr_probs > th,"high","low")
unrestr_cm <- confusionMatrix(factor(unrestr_preds),
                         dtest$quality,
                        positive = "high"
                        )
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])

#restricted model performance
restr_probs <- predict(lr_fit_res,  dtest.res[,-8], type="response")
restr_preds <- ifelse(restr_probs > th,"high","low")
restr_cm <- confusionMatrix(factor(restr_preds),                         dtest$quality,
                        positive = "high"
                        )
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])


data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance), 
           "difference" = unlist(restr_performance)-unlist(unrestr_performance))
```
As expected there is a slight worsening in the model's prediction accuracy, but this increase in bias is negligible with respect to the decrease in variance obtained with the simpler model.

___

## Model diagnostics

Once the model has been trained, we can perform some diagnostics to identify any possible issue with it.

### Multicollinearity 

The Variance Inflation Factor measures how much the behaviour (variance) of an independent variable is influenced, or inflated, by its interaction/correlation with the other independent variables.

```{r}
a <- vif(lr_fit_res)
sqrt(vif(lr_fit_res))>2
```

By checking the VIF after the removal of the variable "density" we can confirm that no other variable is influenced by the others.

### Detecting outliers

Outliers and high leverage points may distort the outcome and accuracy of our logistic regression. In order to find influential outliers we use the Cook's Distance, which measures how much deleting each given observation affects the model.\
As a general rule of thumb, the observations with a Cook’s Distance higher than 3 times the mean - μ - are possible outliers.

```{r warning=FALSE}
# plot Cook's distance
plot(lr_fit_res, which = 4, id.n = 1)
```

We remove the observation with a very large Cook's Distance (id = 4746), and inspect the plot again, to see the variability of the remaining computed distances.

```{r}
dtrain_of <- dtrain.res %>% filter(id != "4746")

lr_fit_of_1 <- glm(quality ~., data =  dtrain_of[,-8],
          family=binomial(link='logit'))

plot(lr_fit_of_1, which = 4, id.n = 0)
```
From this second plot we can see that there are some observations with a larger Cook's Distance with respect to the others, but they are within an acceptable range of variability, and therefore we keep them.

___

## Outliers free model

Now, we build a new model without the outlying observation.
```{r}
#fitting on outlier free data
lr_fit_outlierfree <- glm(quality ~., data =  dtrain_of[,-8],family=binomial(link='logit'))

# coefficients
summary(lr_fit_outlierfree)

# odds ratios
exp(cbind(OR = coef(lr_fit_outlierfree), confint(lr_fit_outlierfree)))
```


### Estimated coefficients

To see the impact that the removed observation had on our estimated model, we can see how much the coefficients changed from model to model.
```{r}
base <- data.frame(summary(lr_fit_res)$coefficient)
no_outliers<- data.frame(summary(lr_fit_outlierfree)$coefficient)

variable<-c("(constant)",colnames(dtrain.res)[-c(7,8)])

data.frame(variable,base$Estimate,no_outliers$Estimate, "difference"=base$Estimate-no_outliers$Estimate)
```
We can notice that the coefficients don't change that much when removing the outlier.\
The biggest changes are in the intercept and in "sulphates", respectively with a -11% and a +1,5% change. We could assume that the outlier we removed affected the intercept coefficient estimate the most.

___

# Bagging

In our analysis the labels are imbalanced, so we try to use bagging to solve the situation and improve our model.\
We resample 10 subsets from training set, train the model we have found in previous step on them and aggregate the 10 models.\ 
From the result, we can see that the accuracy improves.
```{r}
n <- seq(nrow(dtrain_of))
set.seed(1)
lr_b <- function(i){
  set.seed(i)
  s = sample(n, nrow(dtrain_of), replace = TRUE)
  lr_f <- glm(quality ~ fixed.acidity+volatile.acidity+residual.sugar+free.sulfur.dioxide+sulphates+alcohol, 
                  data =  dtrain_of[s, ],family=binomial(link='logit'))
  m = as.data.frame(lr_f$coefficients)
  return(m)
}
co <- data.frame("sub1" = lr_b(1), "sub2" = lr_b(2), "sub3" = lr_b(3), "sub4" = lr_b(4), "sub5" = lr_b(5), "sub6" = lr_b(6), "sub7" = lr_b(7),"sub8" = lr_b(8),"sub9" = lr_b(9),"sub10" = lr_b(10))
co$mean <- rowMeans(co)
pro <- function(i){ # predict the probability
  l_p = co$mean[1]+co$mean[2]*dtest.res[i,1]+co$mean[3]*dtest.res[i,2]+co$mean[4]*dtest.res[i,3]+co$mean[5]*dtest.res[i,4]+co$mean[6]*dtest.res[i,5]+ co$mean[7]*dtest.res[i,6] # linear predictor
  lr_b_prob = exp(l_p)/(1+exp(l_p))
  lr_b_pred <- ifelse(lr_b_prob < th,"high","low")
  return(lr_b_prob)
}
pre <- function(i){
  lr_b_pred <- ifelse(pro(i) < th,"high","low")
  return(lr_b_pred)
}

 

pred <- numeric()
prob <- numeric()
for (i in rownames(dtest.res)){
  p = pro(i)
  prob[i] <- p
  q = pre(i)
  pred[i] <- q
}
cm_b<-confusionMatrix(as.factor(pred),dtest.res$quality, positive = "high")
cm_b
dtest.res$prediction <- pred
dtest.res$probability <- prob
dtest.res$label <- as.numeric(case_when(dtest.res$quality == "low" ~ 1, dtest.res$quality == "high"  ~ 0))
# ROC
library(precrec)
precrec_obj <- evalmod(scores = dtest.res$probability, labels = dtest.res$label)
autoplot(precrec_obj)
 
```

