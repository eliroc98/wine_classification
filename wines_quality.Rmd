---
title: "wine_quality"
author: "LJSSE"
date: "19/5/2021"
output: html_document
---

### Importing packages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(ggplot2)
library(dplyr)
library(reshape2)
library(mlr3learners)
library(mlr3extralearners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
```

################################################################################
## DATA

The dataset **white_wines.csv** stores data about physicochemical properties of many wines which come from the north-west region, named Minho, of Portugal.

Each row refers to a wine and each column contains one of the physicochemical properties with respect to wines. Here we list the properties:

- fixed acidity (FA: g(tartaric acid)/dm^3)
- volatile acidity (VA: g(acetic acid)/dm^3)
- citric acid (CA: g/dm^3)
- residual sugar (RS: g/dm^3)
- chlorides (CH: g(sodium chloride)/dm^3)
- free sulfur dioxide (FSD: mg/dm^3)
- total sulfur dioxide (TSD: mg/dm^3)
- density (DE: g/dm^3)
- pH
- sulphates (SU: g(potassium sulphate)/dm^3)
- alcohol (AL: %vol)
- quality (from 0 to 10)

Our aim will be to classify white wines according to their quality, labeling them as "low" if their quality score is from 0 to 5, and as "high" if the score is between 6 and 10.


## Loading the dataset

```{r, message=FALSE, warning = FALSE}
wwines <- read.csv("winequality-white.csv", sep=";")
print("White wines")
str(wwines)
```

Now we inspect the variables we have and we transform them if necessary.

## Defining the target: high and low quality

```{r, message=FALSE, warning = FALSE}
wwines$quality <- as.factor(case_when(wwines$quality <= 5 ~ "low",
                                      wwines$quality > 5 ~ "high"))
wwines$quality <- factor(wwines$quality, levels = c("low","high"))
table(wwines$quality)

```


## Data description

### Heatmap
```{r}
#compute on all features (without output variable which is at index 12)
cors <- cor(wwines[-12])
corrplot(cors, type = "upper", 
         tl.col = "black", tl.srt = 45)
```

We have to worry about two correlations: residual.sugar with density and density with alcohol.

### Boxplots
```{r, message=FALSE, warning = FALSE}
summary(wwines)
boxplot(wwines$fixed.acidity ~ wwines$quality)
boxplot(wwines$volatile.acidity ~ wwines$quality)
boxplot(wwines$citric.acid ~ wwines$quality)
boxplot(wwines$residual.sugar ~ wwines$quality)
boxplot(wwines$chlorides ~ wwines$quality)
boxplot(wwines$free.sulfur.dioxide ~ wwines$quality)
boxplot(wwines$total.sulfur.dioxide ~ wwines$quality)
boxplot(wwines$density ~ wwines$quality)
boxplot(wwines$pH ~ wwines$quality)
boxplot(wwines$sulphates ~ wwines$quality)
boxplot(wwines$alcohol ~ wwines$quality)

```

Thanks to these plots we can suspect which variables will be significant to perform out regression. Here none of the variables seems to be too different comparing the two subsets (high and low quality).

### Histograms
```{r}
h1 <- ggplot(wwines, aes(fixed.acidity, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h2 <- ggplot(wwines, aes(volatile.acidity, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h3 <- ggplot(wwines, aes(citric.acid, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h4 <- ggplot(wwines, aes(residual.sugar, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

grid.arrange(h1,h2,h3,h4, ncol=1)

```

```{r}
h1 <- ggplot(wwines, aes(chlorides, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h2 <- ggplot(wwines, aes(free.sulfur.dioxide, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h3 <- ggplot(wwines, aes(total.sulfur.dioxide, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h4 <- ggplot(wwines, aes(density, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

grid.arrange(h1,h2,h3,h4, ncol=1)
```

```{r}
h1 <- ggplot(wwines, aes(pH, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h2 <- ggplot(wwines, aes(sulphates, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

h3 <- ggplot(wwines, aes(alcohol, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)


grid.arrange(h1,h2,h3, ncol=1)
```

###########################################################################

# LOGISTIC REGRESSION 

## Divide the dataset in training and test

```{r}
set.seed(42)
split_train_test <- createDataPartition(wwines$quality,p=0.7,list=FALSE)
dtrain<- wwines[split_train_test,]
dtest<-  wwines[-split_train_test,]
```

## Complete model: all variables are included

Estimated coefficients and Odds Ratios.

```{r, message=FALSE, warning=FALSE}
lr_fit <- glm(quality ~., data = dtrain,
          family=binomial(link='logit'))

# coefficients
summary(lr_fit)

# odds ratios
exp(cbind(OR = coef(lr_fit), confint(lr_fit)))
```
Here we find these variable which are significant at 0% confidence interval:
- volatile.acidity: a unitary increase in volatile acidity will lead to a 99.84478% decrease in the odds of having a low quality wine.
- residual.sugar: a unitary increase in volatile acidity will lead to a 15.7744% increase in the odds of having a low quality wine.
- sulphates: a unitary increase in volatile acidity will lead to a 407.3654% increase in the odds of having a low quality wine.
- alcohol: a unitary increase in volatile acidity will lead to a 131.1127% increase in the odds of having a low quality wine.
- free.sulfur.dioxide: a unitary increase in volatile acidity will lead to a 1.2344% increase in the odds of having a low quality wine.

We find these variable which are significant at 5% confidence interval:
- (Intercept)
- density: a unitary increase in volatile acidity will lead to a 100% decrease in the odds of having a low quality wine.

We find this variable which is significant at 10% confidence interval:
- pH: a unitary increase in volatile acidity will lead to a 104.4567% increase in the odds of having a low quality wine.
- total.sulfur.dioxide: a unitary increase in volatile acidity will lead to a 0.24866% decrease in the odds of having a low quality wine.

### Finding the optimal threshold for discriminating high quality wines from low quality wines

```{r}
#here we use mlr3 package
install_learners('classif.log_reg')
mlr_learners$get("classif.log_reg")

#defining our task: classification on wines with quality as response variable and low quality as baseline
task_train = TaskClassif$new(id = "wines", dtrain, target = "quality", positive = "low")
#defining the learner which will perform the training procedure on our model
learner_train = lrn("classif.log_reg", predict_type = "prob")
#defining how we want to perform cross validation (setting number of folds to 5)
cv5 = rsmp("cv", folds = 5)
#we want to tweak the threshold of our classification: we analyze values from 0.3 to 0.6 with step 0.01
thresholds <- seq(0.3,0.6,0.01)

#collecting performances with different thresholds
accuracy <- list()
sensitivity <- list()
specificity <- list()
auroc <- list()
i <- 1
for (thresh in thresholds) {
    res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
    #combined prediction of all individual resampling iterations
    prediction <- res_cv$prediction()
    prediction$set_threshold(thresh)
    #scores are combined as well from all individual resampling iterations
    accuracy[i] <- prediction$score(measures = msr("classif.acc"), task = task_train)
    sensitivity[i] <- prediction$score(measures = msr("classif.sensitivity"), task = task_train)
    specificity[i] <- prediction$score(measures = msr("classif.specificity"), task = task_train)
    auroc[i] <- prediction$score(measures = msr("classif.auc"), task = task_train)
    i<-i+1
}

#collecting scores
measures <- data.frame(thresholds,
                       "accuracy" =unlist(accuracy), 
                       "sensitivity" =unlist(sensitivity),
                       "specificity" = unlist(specificity),
                       "auroc" = unlist(auroc))
  
#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) + 
  geom_line() +
  geom_vline(xintercept = mean(thresholds[intersection_indices]),linetype = "dotted") +
  geom_label(aes(x = 0.37, y = 0.5, label = "0.355"))

```

Optimal threshold is 0.355.

### Complete model test performance 
```{r}
# define new task for dtest
task_test = TaskClassif$new(id = "wines_test", dtest, target = "quality", positive = "low")
# train on train task
learner_train$train(task_train)
# predict on test task
pred <- learner_train$predict(task_test)
# set optimal threshold
pred$set_threshold(0.355)
# performance
cm <- list("confusion" = pred$confusion,
          "accuracy" = pred$score(measures = msr("classif.acc")),
          "sensitivity"=pred$score(measures = msr("classif.sensitivity")),
          "specificity"=pred$score(measures = msr("classif.specificity")))
cm


```

## Model selection

### vif
```{r}
vif(lr_fit)
sqrt(vif(lr_fit))>2 # residual.sugar, density, alcohol
```

Variance inflation factor measures how much the behavior (variance) of an independent variable is influenced, or inflated, by its interaction/correlation with the other independent variables.

Residual sugar, density and alcohol are the most influenced by theit interactions with other independent variables.

### Best Subset regression
```{r}
library(leaps)
regfit.full=regsubsets(quality~.,data=wwines, nvmax=13)
reg.summary=summary(regfit.full)
#names(reg.summary)

plot(reg.summary$bic,xlab="Number of Variables",ylab="bic")
points(which.min(reg.summary$bic),reg.summary$bic[which.min(reg.summary$bic)],pch=20,col="red")

```
BIC says that the best number of independent variables to select is 7 (having BIC value -1245.331).

```{r}
plot(regfit.full,scale="bic")
grid(nx=12,ny=11,col="red",lty = "solid")
abline(h=7, col="yellow", lwd = 10)
text(11,8, "Optimal model", col="yellow")
```

### Forward Stepwise method
```{r}
regfit.fwd=regsubsets(quality~.,data=wwines,method="forward", nvmax=13)
summary(regfit.fwd)
```
Forward stepwise steps:
1. inclusion of alcohol
2. inclusion of volatile.acidity
3. inclusion of residual.sugar
4. inclusion of sulphates
5. inclusion of density
6. inclusion of pH
7. inclusion of free.sulfur.dioxide
8. inclusion of total.sulfur.dioxide
9. inclusion of fixed.acidity
10. inclusion of citric.acid
11. inclusion of chlorides

```{r}
plot(regfit.fwd,scale="bic")
grid(nx=12,ny=11,col="red",lty = "solid")
abline(h=which.min(summary(regfit.fwd)$bic), col="yellow", lwd = 10)
text(11,8, "Optimal model", col="yellow")
```


## Restricted model: only 7 variables

Only including: volatile.acidity, residual.sugar, free.sulfur.dioxide, density, pH, sulphates, alcohol.
```{r}
lr_fit_sig <- glm(quality ~ volatile.acidity+residual.sugar+free.sulfur.dioxide+density+pH+sulphates+alcohol, data = dtrain,
          family=binomial(link='logit'))

# coefficients
summary(lr_fit_sig)

# odds ratios
exp(cbind(OR = coef(lr_fit_sig), confint(lr_fit_sig)))
```
Here we find these variable which are significant at 0% confidence interval:
- (Intercept)
- volatile.acidity: a unitary increase in volatile acidity will lead to a 99.86127% decrease in the odds of having a low quality wine.
- residual.sugar: a unitary increase in volatile acidity will lead to a 17.2293% increase in the odds of having a low quality wine.
- free.sulfur.dioxide: a unitary increase in volatile acidity will lead to a 0.9025% increase in the odds of having a low quality wine.
- density: a unitary increase in volatile acidity will lead to a 100% decrease in the odds of having a low quality wine.
- sulphates: a unitary increase in volatile acidity will lead to a 377.0417% increase in the odds of having a low quality wine.
- alcohol: a unitary increase in volatile acidity will lead to a 124.092% increase in the odds of having a low quality wine.

We find these variable which are significant at 1% confidence interval:
- pH: a unitary increase in volatile acidity will lead to a 125.6878% increase in the odds of having a low quality wine.

## Test performances of restricted and unrestricted models
```{r, warning=FALSE}
#unrestricted model performance
unrestr_probs <- predict(lr_fit, dtest, type="response")
unrestr_preds <- ifelse(unrestr_probs > 0.355,"low","high")
unrestr_cm <- confusionMatrix(as.factor(unrestr_preds),
                        dtest$quality,
                        positive = "low"
                        )
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])

#restricted model performance
restr_probs <- predict(lr_fit_sig, dtest, type="response")
restr_preds <- ifelse(restr_probs > 0.355,"low","high")
restr_cm <- confusionMatrix(as.factor(restr_preds),
                        dtest$quality,
                        positive = "low"
                        )
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])


data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance), 
           "improvement" = unlist(restr_performance)-unlist(unrestr_performance))
```
There is no improvement in using the restricted model rather than the unrestricted.

## Detecting outliers (first)
```{r, warning=FALSE}
#see Cook's distance
influenceIndexPlot(lr_fit, id.n=10)  # 2782, 4481
#using dffits
df_fits<-dffits(lr_fit)
threshold<-(2*(11+1)/sqrt(3429-11-1))  
which(abs(df_fits)>threshold) # 2782, 1956
outliers <- c(1956,2782,4481)
dtrain["outlier"] <- ifelse(rownames(dtrain) %in% outliers,0,1)
residualPlots(lr_fit, id = TRUE,  col = as.factor(dtrain$outlier))
```
From the first plot we identify as outliers instances number 2782 and 4481 (see Cook's distance plot). These instances are outliers for the independent variables distributions.

2782 and 1956 are outliers for the distribution of the response variable (dffits).

## Detecting outliers using unrestricted model (second)
```{r, warning=FALSE}
library(broom)
#dffits threshold
threshold<-(2*(ncol(dtrain)+1)/sqrt(nrow(dtrain)-ncol(dtrain)-1))  
d <- dffits(lr_fit)
#another way to get cook's distance
#cooks distance threshold: https://www.scikit-yb.org/en/latest/api/regressor/influence.html#:~:text=Because%20of%20this%2C%20Cook's%20Distance,that%20is%20above%20that%20threshold.
plot(lr_fit, which = 4, id.n = 3)
outliers <- 
  augment(lr_fit) %>%
  mutate(df = unname(dffits(lr_fit)))%>%
  filter(.cooksd > 4/nrow(dtrain) | abs(df)>threshold) %>%
  select(.rownames)

dtrain["outlier"] <- ifelse(rownames(dtrain) %in% outliers$.rownames,0,1)
residualPlots(lr_fit,  col = as.factor(dtrain$outlier))
```


## Outliers free model (first)
```{r}
dtrain$outlier<- NULL
dtrain_outlierfree <- dtrain[-outliers,]
lr_fit_outlierfree <- glm(quality ~., data = dtrain_outlierfree,
                          family=binomial(link='logit'))

# coefficients
summary(lr_fit_outlierfree)

# odds ratios
exp(cbind(OR = coef(lr_fit_outlierfree), confint(lr_fit_outlierfree)))
```

## Outliers free model (second)
```{r}
dtrain$outlier<- NULL
dtrain_outlierfree <- dtrain[-as.numeric(outliers$.rownames),]
lr_fit_outlierfree <- glm(quality ~., data = dtrain_outlierfree,
                          family=binomial(link='logit'))

# coefficients
summary(lr_fit_outlierfree)

# odds ratios
exp(cbind(OR = coef(lr_fit_outlierfree), confint(lr_fit_outlierfree)))
```

## With and without outliers

### Estimated coefficients
```{r}
outliers <- data.frame(summary(lr_fit)$coefficient)
no_outliers<- data.frame(summary(lr_fit_outlierfree)$coefficient)

variable<-c("(constant)",colnames(wwines)[-12])

data.frame(variable,outliers$Estimate,no_outliers$Estimate, "difference"=outliers$Estimate-no_outliers$Estimate)
```

### Performance on test
```{r}
# outliers free
lr_prob_of <- predict(lr_fit_outlierfree, dtest, type="response")
lr_pred_of <- ifelse(lr_prob_of > 0.355,"low","high")
cm_of <- confusionMatrix(as.factor(lr_pred_of),
                        dtest$quality,
                        positive = "low"
                        )

out_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])
outfree_performance <- list(cm_of$overall[1], cm_of$byClass[1], cm_of$byClass[2])

data.frame("outliers_performance" = unlist(out_performance),"outliers_free_performance" = unlist(outfree_performance), 
           "improvement" = unlist(outfree_performance)-unlist(out_performance))
```

# Outliers free ROC Curve

```{r, message=FALSE, warning=FALSE}
test_roc = roc(dtest$quality ~ lr_prob_of, plot = TRUE, print.auc = TRUE)

```

################################################################################
# BAGGING 
```{r}
n <- seq(nrow(dtrain))
set.seed(1)
S1 <- sample(n, nrow(dtrain), replace = TRUE)
set.seed(2)
S2 <- sample(n, nrow(dtrain), replace = TRUE)
set.seed(3)
S3 <- sample(n, nrow(dtrain), replace = TRUE)
set.seed(4)
S4 <- sample(n, nrow(dtrain), replace = TRUE)
lr_b <- function(i){
      lr_f <- glm(quality ~ volatile.acidity+residual.sugar+free.sulfur.dioxide+density+pH+sulphates+alcohol, 
                  data = dtrain[i, ],family=binomial(link='logit'))
      lr_prob <- predict(lr_f,dtest[i,], type="response")
      lr_pred <- ifelse(lr_prob > 0.355,"low","high")
      cm <- confusionMatrix(
                as.factor(lr_pred),
                dtest[i,]$quality,
                positive = "low"
                )
      a <- cm$overall[[1]]
      return(a)
}
aggregate = (lr_b(S1) +lr_b(S2) + lr_b(S3) + lr_b(S4))/4
round(aggregate,4)
```

################################################################################

# Relation with restricted model variables

volatile.acidity, residual.sugar, free.sulfur.dioxide, density, 
pH, sulphates, alcohol


```{r}
ggplot(wwines, aes(x = volatile.acidity, color = quality)) +
  geom_histogram() +
  labs(x = "volatile.acidity",
       title = "Wine quality by volatile.acidity")

ggplot(wwines, aes(x = residual.sugar, color = quality)) +
  geom_histogram() +
  labs(x = "residual.sugar",
       title = "Wine quality by residual.sugar")

ggplot(wwines, aes(x = free.sulfur.dioxide, color = quality)) +
  geom_histogram() +
  labs(x = "free.sulfur.dioxide",
       title = "Wine quality by free.sulfur.dioxide")

ggplot(wwines, aes(x = density, color = quality)) +
  geom_histogram() +
  labs(x = "density",
       title = "Wine quality by density")

```
