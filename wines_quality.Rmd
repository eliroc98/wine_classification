---
title: "White wines' quality classification with Logistic Regression"
author: "LJSSE"
date: "19/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(reshape2)
library(mlr3learners)
library(pROC)
library(corrplot)
library(gridExtra)
library(GGally)
library(car)
library(broom)
library(ltm)
library(MASS)
library(leaps)
library(ROCit)
```

![](wines.jpeg)

# Introduction

The dataset **white_wines.csv** stores data about physicochemical properties of many white wines coming from the north-west region of Portugal, called Minho.

Each row refers to a wine, while each column contains a physicochemical property.\
Here we list the properties and their unit of measure:

* fixed acidity (FA: g(tartaric acid)/dm^3)
* volatile acidity (VA: g(acetic acid)/dm^3)
* citric acid (CA: g/dm^3)
* residual sugar (RS: g/dm^3)
* chlorides (CH: g(sodium chloride)/dm^3)
* free sulfur dioxide (FSD: mg/dm^3)
* total sulfur dioxide (TSD: mg/dm^3)
* density (DE: g/dm^3)
* pH
* sulphates (SU: g(potassium sulphate)/dm^3)
* alcohol (AL: %vol)
* *target*: quality (from 0 to 10)

The aim of this project will be to classify white wines according to their quality, labeling them as "low" if their quality score is from 0 to 5, and as "high" if the score is between 6 and 10.

____

# The data

First, we nedd to inspect the variables and make the suitable transformations, if necessary.

```{r, message=FALSE, warning = FALSE}
wwines <- read.csv("winequality-white.csv", sep=";")
str(wwines)
```

All the features are numeric, and are already stored in the right format.\

## Defining the target

We re-encode "quality" as a factor with two levels, in order to make the target binary. We also specify the order of the levels, since the method to perform logistic regression takes the first level as the baseline, and we want to predict the probability of having a high quality wine.\
As we can see the two categories are unbalanced, with the data containing a higher proportion of "high" instances.

```{r, message=FALSE, warning = FALSE}
wwines$quality <- factor(case_when(wwines$quality <= 5 ~ "low",
                                      wwines$quality > 5 ~ "high"), levels = c("low", "high")) # relevel
table(wwines$quality)
```

In order to have a unique identifier for each wine, we also create an "id" variable.

```{r}
wwines["id"] <- rownames(wwines)
```

___

# Data description

Before proceeding with our analysis, we perform some descriptive statistics in order to both understand the relations within the variables, and to visualize their distributions. This will help us to get a first idea of the relevance of the features for our classification task.

### Heatmap

First we plot the correlation matrix, in order to detect any strong relation among features which could lead to multicollinearity problems.
```{r}
cors <- cor(wwines[-c(12,13)])
corrplot(cors, type = "upper",tl.col = "black", tl.srt = 45)
```

This plot highlights a strong correlation between density and two variables: residual.sugar and alcohol.

### Frequency polygons

To get an understanding of which variable should be excluded from the regression, we plot the histograms of the highly correlated variables identified above.
```{r}
residual.sugar.plot <- ggplot(wwines, aes(residual.sugar, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

alcohol.plot <- ggplot(wwines, aes(alcohol, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

density.plot <- ggplot(wwines, aes(density, color = quality))+
  geom_freqpoly(binwidth = 3, size = 1)

grid.arrange(residual.sugar.plot,alcohol.plot,density.plot, ncol=1)

```

We decide to remove density.\
In fact, we can observe that for each value of density there is no difference in the distribution of quality, once we account for the numerosity of the two quality levels. On the other hand, the modal values for the other two variables differ for low and high quality wines, thus adding more useful information for the purpose of classification.

```{r}
wwines$density <- NULL
```


### Correlation with the target 

It is also useful to compute the correlations between the covariates and the target, in order to see if there are any particularly powerful features that would make the task too trivial.\
In order to do so we exploit *biserial correlation*, which allows to compute a correlation index between a numerical and a binary variable.
```{r, warning=FALSE, message=FALSE}
bicorrs <- sapply(wwines[,-c(11,12)], function(x) round(biserial.cor(x,wwines$quality),2))
data.frame(bicorrs) %>% arrange(abs(bicorrs))
```
Here the correlations are displayed in ascending absolute value, and we can observe that the variable showing the highest correlation with wines quality is alcohol, but it is not too strong (-38%).
The sign is negative, meaning that, in our data, wines with a greater alcohol percentage are likely to be of higher quality.

### Boxplots

We now look at the distribution of all the explanatory variables, in order to both visualize those values which are far from the bulk of the data, and to get an understanding of the variables displaying the most divergent median values among low and high quality wines.

```{r, message=FALSE, warning = FALSE}
# melt the dataset
wwines.m <- melt(wwines[,-12], id.var = "quality")

# wrapped boxplots
ggplot(data = wwines.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=quality)) +
facet_wrap( ~ variable, scales="free")
```

We can observe the following:

* The features displaying the highest median difference between high and low quality wines are "volatile.acidity", "total.sulfur.dioxide", "pH", "alcohol." Theese variables are candidates to be the significant ones in the regression framework.
* There are some outlying values. We will deal with them in the section below.

___

### Data splitting

In order to train and validate our model on different sets, we split the data, keeping 30% of it to carry out performance evaluation.

```{r}
set.seed(42)
train_indices<- createDataPartition(wwines$quality,p=0.7,list=FALSE)
dtrain <- wwines[train_indices,]
dtest <- wwines[-train_indices,]
```

____

# Variables Selection

### Forward Stepwise Method

To find the main factors determining wine quality and to remove the unrelated ones, we use forward stepwsie method to do variable selection.\
The best subset of the model can be found by starting from a null model with no predictors, then picking the best p (p is the number of predictors in the full model) models which have exactly 1, 2, ..., p predictors, and finally selecting the model with the lowest Bayesian information criterion (BIC) value.\
In our case, the inclusion of the variables follows the order of alcohol, volatile.acidity, residual.sugar, sulphates, fixed.acidity, free.sulfur.dioxide, total.sulfur.dioxide, pH, chlorides and citric.acid.\
When the model contains 6 variables, it has lowest BIC value. So, the variables of the optimal model are the six first ones.

```{r}
regfit.fwd=regsubsets(quality~.,data=wwines[,-12],method="forward", nvmax=13)
summary.fwd <- summary(regfit.fwd)

plot(summary.fwd$bic,xlab="Number of Variables",ylab="bic")
points(which.min(summary.fwd$bic),summary.fwd$bic[which.min(summary.fwd$bic)],pch=20,col="red")
```

Optimal number of variables: 6.


We plot the results in order to visualize which variables have been selected with BIC in each model, and we highlight the optimal model with a yellow line. 

```{r}
plot(regfit.fwd,scale="bic")
grid(nx=11,ny=10,col="red",lty = "solid")
abline(h=10, col="yellow", lwd = 10)
text(10.5,9, "Optimal model", col="yellow",cex = 0.8)
```

___

# Logistic Regression

Logistic Regression is now performed on the restricted model with the six selected variables:\
fixed.acidity, volatile.acidity, residual.sugar, free.sulfur.dioxide, sulphates, alcohol.

```{r}
# remove unused variables
dtrain.res <- dtrain[,c(1,2,4,6,9,10,11,12)]
dtest.res <- dtest[,c(1,2,4,6,9,10,11,12)]

# train the model
lr_fit_res <- glm(quality ~., data =  dtrain.res[,-8],
          family=binomial(link='logit'))

# coefficients
summary(lr_fit_res)

# odds ratios
exp(cbind(OR = coef(lr_fit_res), confint(lr_fit_res)))
```

The following variables are those significant at a 99% confidence level:\

* *"Intercept"*: the decrease in the odds of having a high quality wine when all the explanatory variables are set to 0 is 99.98%.
* *"fixed.acidity"*: a unitary increase (1 g/dm^3) in fixed acidity will lead to a 18.1% decrease in the odds of having a high quality wine.
* *"volatile.acidity*": a unitary increase (1 g/dm^3) in volatile acidity will lead to a 99.89% decrease in the odds of having a high quality wine.
* *"residual.sugar"*: a unitary increase (1 g/dm^3) in residual sugar will lead to a 7.2% increase in the odds of having a high quality wine.
* *"free.sulfur.dioxide"*: a unitary increase (1 mg/dm^3) in free sulfur dioxide will lead to a 0.92% increase in the odds of having a high quality wine.
* *"alcohol"*: a unitary increase (percentage) in alcohol will lead to a 196.73% increase in the odds of having a high quality wine.

The following variable is significant at a 95% confidence level:\

* *"sulphates"*: a unitary increase (1 g/dm^3) in sulphates will lead to a 223.72% increase in the odds of having a high quality wine.
  
From the regression output we can state that only a higher fixed and volatile acidity decrease the probability of a wine being of high quality, while residual sugar, free sulfur dioxide, sulphates, and alcohol all increase the chances of a wine being classified as "high", with the last two features being the most relevant (the ones with the highest coefficients).



### Optimal threshold

In order to get the best possible performance, we perform threshold optimization exploiting the `mlr3 package`. 

```{r,results='hide'}
# task: classification of training data
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
# type of learner: logistic regression
learner_train = lrn("classif.log_reg", predict_type = "prob")
# resampling method: cross validation
cv5 = rsmp("cv", folds = 5)
# defing the search space
thresholds <- seq(0.5,0.8,0.01)

#collecting cross-validated performances for different thresholds
measures_list <- rep(list(list()), 3)

for (thresh in thresholds) {
    res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
    #combined prediction of all individual resampling iterations
    prediction <- res_cv$prediction()
    prediction$set_threshold(thresh)
    #computing the scores
    scores <- prediction$score(measures = c(msr("classif.acc"),msr("classif.sensitivity"),msr("classif.specificity")))
    coefficients <- (unname(scores))
    measures_list <- mapply(append, measures_list, coefficients, SIMPLIFY = FALSE)
}

```

Since the choice of the threshold strictly depends on the aim of the analysis, which in our case is to build a predictor which would have the best behaviour in classifying new different data, we look for a threshold which could guarantee an equilibrium between the two types of errors.

```{r}
measures <- data.frame(thresholds,
                       "accuracy" = unlist(measures_list[[1]]), 
                       "sensitivity" = unlist(measures_list[[2]]),
                       "specificity" = unlist(measures_list[[3]]))


#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
th <- mean(thresholds[intersection_indices]) 

melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) + 
  geom_line() +
  geom_vline(xintercept = th,linetype = "dotted") +
  geom_label(aes(x = 0.67, y = 0.5, label = as.character(mean(thresholds[intersection_indices]))))
```

The optimal threshold for our aim is 0.645.\
This threshold meets our expectations, as the equilibrium point corresponds to a value higher than 0.5. \
This higher threshold compensates for the higher a priori probability of labelling a wine as high quality (low: 1640, high: 3258).



### Model's test performance 

To evaluate the performance of the estimated model, we use the Confusion Matrix to compute the accuracy, sensitivity and sensibility scores.

```{r}
task_train = TaskClassif$new(id = "wines",  dtrain.res[,-8], target = "quality", positive = "high")
task_test = TaskClassif$new(id = "wines_test",  dtest.res[,-8], target = "quality", positive = "high")

learner = lrn("classif.log_reg", predict_type = "prob")
cv5 = rsmp("cv", folds = 5)

learner$train(task_train) # training on train set
base_pred <- learner$predict(task_test) # predicting on test set
base_pred$set_threshold(th)

# performance
cm_base <- list("confusion" = base_pred$confusion,
          "accuracy" = base_pred$score(measures = msr("classif.acc")),
          "sensitivity"=base_pred$score(measures = msr("classif.sensitivity")),
          "specificity"=base_pred$score(measures = msr("classif.specificity")))
cm_base
```

We can state that:

* the model predicts wines quality with a 71.55% accuracy.
* 71.65% of wines are correctly identified as high quality wines.
* 71.34% of wines are correctly identified as low quality wines.



### Full/Restricted Performance comparison 

In order to see how the model's performance is affected by the dimensionality reduction, we compute the predictions for both the restricted and the full model and we compare the values of accuracy, sensitivity, and specificity.
By removing 5 out of 11 explanatory variables we are trying to avoid overfitting, thus creating a more stable model with a lower variance; of course, we expect the performance to be slightly worse, as we are introducing some bias.


```{r, warning=FALSE}
#running the full model

lr_fit <- glm(quality ~., data =  dtrain[,-12],
          family=binomial(link='logit'))

#full model performance
unrestr_probs <- predict(lr_fit,  dtest[,-12], type="response")
unrestr_preds <- ifelse(unrestr_probs > th,"high","low")
unrestr_cm <- confusionMatrix(factor(unrestr_preds),
                         dtest$quality,
                        positive = "high"
                        )
unrestr_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])

#restricted model performance
restr_probs <- predict(lr_fit_res,  dtest.res[,-8], type="response")
restr_preds <- ifelse(restr_probs > th,"high","low")
restr_cm <- confusionMatrix(factor(restr_preds),                         dtest$quality,
                        positive = "high"
                        )
restr_performance <- list(restr_cm$overall[1], restr_cm$byClass[1], restr_cm$byClass[2])


data.frame("unrestr_performance" =unlist(unrestr_performance),"restr_performance" = unlist(restr_performance), 
           "difference" = unlist(restr_performance)-unlist(unrestr_performance))
```

As expected there is a slight worsening in the model's prediction accuracy, but this increase in bias is negligible with respect to the decrease in variance obtained with the simpler model.

___

## Model diagnostics

Once the model has been trained, we can perform some diagnostics to identify any possible issue with it.

### Multicollinearity 

The Variance Inflation Factor measures how much the behaviour (variance) of an independent variable is influenced, or inflated, by its interaction/correlation with the other independent variables.

```{r}
a <- vif(lr_fit_res)
sqrt(vif(lr_fit_res))>2
```

By checking the VIF after the removal of the variable "density" we can confirm that no other variable is influenced by the others.

### Detecting outliers

Outliers and high leverage points may distort the outcome and accuracy of our logistic regression. In order to find influential outliers we use the Cook's Distance, which measures how much deleting each given observation affects the model.\
As a general rule of thumb, the observations with a Cookâ€™s Distance higher than 4/(N-k-1) are possible outliers, where N is the number of observations and k is the number of covariates.

```{r warning=FALSE}
# plot Cook's distance
cd <- cooks.distance(lr_fit_res)
N <- 3429
k <- 6
threshold <-  4/(N-k-1)
plot(lr_fit_res, which = 4, id.n = 0)
abline(h=threshold, col="red")
```

We remove the observations with a Cook's Distance larger than the defined threshold and inspect the plot again, to see the variability of the remaining computed distances.

```{r}
dtrain_of <- dtrain.res[-which(cd>threshold),]
dtest_of <- dtest.res[-which(cd>threshold),]

lr_fit_of_1 <- glm(quality ~., data =  dtrain_of[,-8],
          family=binomial(link='logit'))

plot(lr_fit_of_1, which = 4, id.n = 0)
```

From this second plot we can see that the Cook's distances are now within an acceptable range of variability.

___

## Outliers free model

Now, we build a new model without the outlying observations.
```{r}
#fitting on outlier free data
lr_fit_outlierfree <- glm(quality ~., data =  dtrain_of[,-8],family=binomial(link='logit'))

# coefficients
summary(lr_fit_outlierfree)

# odds ratios
exp(cbind(OR = coef(lr_fit_outlierfree), confint(lr_fit_outlierfree)))
```


### Threshold tuning

We perform threshold tuning again for the new model.

```{r,results='hide'}
# task: classification of training data
task_train = TaskClassif$new(id = "wines",  dtrain_of[,-8], target = "quality", positive = "high")
# type of learner: logistic regression
learner_train = lrn("classif.log_reg", predict_type = "prob")
# resampling method: cross validation
cv5 = rsmp("cv", folds = 5)
# defing the search space
thresholds <- seq(0.5,0.8,0.01)

#collecting cross-validated performances for different thresholds
measures_list <- rep(list(list()), 3)

for (thresh in thresholds) {
    res_cv = resample(task_train, learner_train, cv5, store_models = TRUE)
    #combined prediction of all individual resampling iterations
    prediction <- res_cv$prediction()
    prediction$set_threshold(thresh)
    #computing the scores
    scores <- prediction$score(measures = c(msr("classif.acc"),msr("classif.sensitivity"),msr("classif.specificity")))
    coefficients <- (unname(scores))
    measures_list <- mapply(append, measures_list, coefficients, SIMPLIFY = FALSE)
}

```

```{r}
measures <- data.frame(thresholds,
                       "accuracy" = unlist(measures_list[[1]]), 
                       "sensitivity" = unlist(measures_list[[2]]),
                       "specificity" = unlist(measures_list[[3]]))


#finding optimal point: intersection
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices <- which(equivalent(measures$sensitivity,measures$specificity))
th_2 <- mean(thresholds[intersection_indices]) 

melt_measures <- melt(measures, id.vars="thresholds")
ggplot(melt_measures, aes( x=thresholds, y=value, colour=variable, group=variable )) + 
  geom_line() +
  geom_vline(xintercept = th_2,linetype = "dotted") +
  geom_label(aes(x = 0.67, y = 0.5, label = as.character(mean(thresholds[intersection_indices]))))
```

### Change in performance

```{r}
lr_fit <- glm(quality ~., data =  dtrain[,-12],
          family=binomial(link='logit'))

#full model performance
unrestr_probs <- predict(lr_fit,  dtest[,-12], type="response")
unrestr_preds <- ifelse(unrestr_probs > th,"high","low")
unrestr_cm <- confusionMatrix(factor(unrestr_preds),
                         dtest$quality,
                        positive = "high"
                        )
base_performance <- list(unrestr_cm$overall[1], unrestr_cm$byClass[1], unrestr_cm$byClass[2])

#outlierfree model performance
out_probs <- predict(lr_fit_outlierfree,  dtest_of[,-8], type="response")
out_preds <- ifelse(out_probs > th_2,"high","low")
out_cm <- confusionMatrix(factor(out_preds),dtest_of$quality,
                        positive = "high"
                        )
outliersfree_performance <- list(out_cm$overall[1], out_cm$byClass[1], out_cm$byClass[2])


data.frame("base_performance" =unlist(base_performance),"outliers_free_performance" = unlist(outliersfree_performance), 
           "difference" = unlist(outliersfree_performance) - unlist(base_performance))
```
After removing the outliers, the model has a higher overall accuracy on the test set.

### Estimated coefficients

To see the impact that the removed observation had on our estimated model, we can see how much the coefficients changed from model to model.\
The issue with outliers is in fact that they have a great inpact on the estimation of coefficients.

```{r}
base <- data.frame(summary(lr_fit_res)$coefficient)
no_outliers<- data.frame(summary(lr_fit_outlierfree)$coefficient)

variable<-c("(constant)",colnames(dtrain.res)[-c(7,8)])

data.frame(variable,base$Estimate,no_outliers$Estimate, "difference"=base$Estimate-no_outliers$Estimate)
```
We can notice that the coefficients are quite different after outliers removal, in particular those related to the constant, volatile acidity, sulphates and alcohol.\
However, no coefficient changes sign.

___

# Bagging

To further improve our model, we tried bagging.\
We resample 10 subsets from training set, train the model we have found in previous step on them and aggregate the 10 models to get bagging model.\ 
The optimal threshold we find for bagging model is the same as the one for the previous model, **0.645**.
From the result, we can see that the performance has improved.

* Accuracy: 71.75%
* Sensitivity: 71.95%
* Specificity: 71.34%

```{r}
n <- seq(nrow(dtrain_of))
set.seed(1)
lr_b <- function(i){
  set.seed(i)
  s = sample(n, nrow(dtrain_of), replace = TRUE)
  lr_f <- glm(quality ~ fixed.acidity+volatile.acidity+residual.sugar+free.sulfur.dioxide+sulphates+alcohol, 
                  data =  dtrain_of[s, ],family=binomial(link='logit'))
  m = as.data.frame(lr_f$coefficients)
  return(m)
} 
co <- data.frame("sub1" = lr_b(1), "sub2" = lr_b(2), "sub3" = lr_b(3), "sub4" = lr_b(4), "sub5" = lr_b(5), "sub6" = lr_b(6), "sub7" = lr_b(7),"sub8" = lr_b(8),"sub9" = lr_b(9),"sub10" = lr_b(10))
co$mean <- rowMeans(co) # coefficients of bagging model

# the bagging model

# predict the probability of training
pro <- function(i){ 
  # linear predictor/logit/link
  l_p = co$mean[1]+co$mean[2]*dtrain_of[i,1]+co$mean[3]*dtrain_of[i,2]+co$mean[4]*dtrain_of[i,3]+co$mean[5]*dtrain_of[i,4]+co$mean[6]*dtrain_of[i,5]+ co$mean[7]*dtrain_of[i,6] 
  lr_b_prob = exp(l_p)/(1+exp(l_p))
  return(lr_b_prob)
} 
lr_b_pro <- numeric()
for (i in rownames(dtrain_of)){
  p <- pro(i)
  lr_b_pro[i]<- p
}

lr_b_pre <- numeric()
accuracy <- numeric()
sensitivity <- numeric()
specificity <- numeric()

# optimal threshold
for (i in seq(0.01, 0.98, 0.01)){
  for (j in rownames(dtrain_of)){
    q <- ifelse(lr_b_pro[j] < i,"low","high")
    lr_b_pre[j] <- q
  }
  cm <- confusionMatrix(factor(lr_b_pre, levels = c("low", "high")),factor(dtrain_of$quality, levels = c("low", "high")), positive = "high")
  accuracy[i*100] <- cm$overall["Accuracy"]
  sensitivity[i*100] <- cm$byClass[1]
  specificity[i*100] <- cm$byClass[2]
}

measures_b <- data.frame("thresholds" = seq(0.01, 0.98, 0.01), "accuracy" = accuracy, "sensitivity" = sensitivity, "specificity" = specificity)


#finding optimal point: intersection 0.655
equivalent <- function(x, y, tol = 0.02) abs(x - y) < tol
intersection_indices_b <- which(equivalent(measures_b$sensitivity,measures_b$specificity))
th_b <- mean(measures_b$thresholds[intersection_indices_b])

melt_measures_b <- melt(measures, id.vars="thresholds")
ggplot(melt_measures_b, aes( x=thresholds, y=value, colour=variable, group=variable )) + 
  geom_line() +
  geom_vline(xintercept = th,linetype = "dotted") +
  geom_label(aes(x = 0.67, y = 0.5, label = as.character(th_b)))

# Performance of test set
pro_t <- function(i){ # predict the probability of test
  l_p = co$mean[1]+co$mean[2]*dtest.res[i,1]+co$mean[3]*dtest.res[i,2]+co$mean[4]*dtest.res[i,3]+co$mean[5]*dtest.res[i,4]+co$mean[6]*dtest.res[i,5]+ co$mean[7]*dtest.res[i,6] 
  lr_b_prob = exp(l_p)/(1+exp(l_p))
  return(lr_b_prob)
}
pre_t <- function(i){
  lr_b_pred <- ifelse(pro_t(i) < th_b,"low","high")
  return(lr_b_pred)
}


pred_t <- numeric()
prob_t <- numeric()
for (i in rownames(dtest.res)){
  p = pro_t(i)
  prob_t[i] <- p
  q = pre_t(i)
  pred_t[i] <- q
}
cm_b<-confusionMatrix(factor(pred_t,levels = c("low", "high")),factor(dtest.res$quality,levels = c("low", "high")), positive = "high")
cm_b

dtest.res$prediction <- pred_t
dtest.res$probability <- prob_t
dtest.res$label <- as.numeric(case_when(dtest.res$quality == "low" ~ 0, dtest.res$quality == "high"  ~ 1))

# ROC
ROCit_obj <- rocit(dtest.res$probability,class=dtest.res$label)
plot(ROCit_obj)

# link and response
l_p_t <- numeric()
l_p <- function(i){ # predict the probability
  l_p = co$mean[1]+co$mean[2]*dtest.res[i,1]+co$mean[3]*dtest.res[i,2]+co$mean[4]*dtest.res[i,3]+co$mean[5]*dtest.res[i,4]+co$mean[6]*dtest.res[i,5]+ co$mean[7]*dtest.res[i,6] 
}
for (i in rownames(dtest.res)){
  l = l_p(i)
  l_p_t[i]<- l
}

score_data <- data.frame(link=l_p_t, 
                         response=prob_t,
                         quality=dtest.res$quality,
                         stringsAsFactors=FALSE)
score_data %>% 
  ggplot(aes(x=link, y=response, col=quality)) + 
  scale_color_manual(values=c("black", "red")) + 
  geom_point() + 
  geom_rug()
```

